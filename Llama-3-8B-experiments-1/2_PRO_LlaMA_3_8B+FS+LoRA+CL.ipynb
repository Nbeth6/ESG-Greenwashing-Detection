{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0FCrjvUJwScj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}\")\n",
        "else:\n",
        "  print(\"No GPU detected. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import of A3CG files\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "print(\"IMPORTING A3CG FILES\")\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Finding the fold_1 folder\n",
        "print(\"\\nSearching for the 'fold_1' folder...\")\n",
        "\n",
        "possible_path = \"/content/drive/MyDrive/A3CG_Dataset/folds/fold_1\"\n",
        "drive_fold_path = None\n",
        "\n",
        "if os.path.exists(possible_path):\n",
        "    drive_fold_path = possible_path\n",
        "    print(f\"Found: {possible_path}\")\n",
        "else:\n",
        "    print(\"Folder 'fold_1' not found.\")\n",
        "    exit()\n",
        "\n",
        "# List contents of the folder\n",
        "print(f\"\\nContents of: {drive_fold_path}\")\n",
        "files_in_folder = os.listdir(drive_fold_path)\n",
        "required_files = [\"seen_train.json\", \"seen_val.json\", \"seen_test.json\", \"unseen_test.json\"]\n",
        "\n",
        "for file in files_in_folder:\n",
        "    if file.endswith('.json'):\n",
        "        file_path = os.path.join(drive_fold_path, file)\n",
        "        size_kb = os.path.getsize(file_path) / 1024\n",
        "        status = \"[OK]\" if file in required_files else \"[INFO]\"\n",
        "        print(f\"  {status} {file} ({size_kb:.1f} KB)\")\n",
        "\n",
        "# Create local structure\n",
        "local_path = \"/content/A3CG_DATASET/folds/fold_1\"\n",
        "os.makedirs(local_path, exist_ok=True)\n",
        "print(f\"\\nDirectory created: {local_path}\")\n",
        "\n",
        "# Copy files\n",
        "print(\"\\nCopying files...\")\n",
        "copied_count = 0\n",
        "\n",
        "for filename in required_files:\n",
        "    source = os.path.join(drive_fold_path, filename)\n",
        "    dest = os.path.join(local_path, filename)\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        try:\n",
        "            shutil.copy2(source, dest)\n",
        "            size_kb = os.path.getsize(dest) / 1024\n",
        "\n",
        "            # Check for valid JSON\n",
        "            with open(dest, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            print(f\"  [OK] {filename}: {len(data)} samples ({size_kb:.1f} KB)\")\n",
        "            copied_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [ERROR] {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"  [MISSING] {filename} not found\")\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nRESULT:\")\n",
        "print(f\"  Files copied: {copied_count}/4\")\n",
        "\n",
        "if copied_count >= 3:\n",
        "    print(f\"SUCCESS! Files are ready.\")\n",
        "    print(f\"Path: {local_path}\")\n",
        "\n",
        "    # Show final structure\n",
        "    print(f\"\\nFinal Directory Structure:\")\n",
        "    for root, dirs, files in os.walk(\"/content/A3CG_DATASET\"):\n",
        "        level = root.replace(\"/content/A3CG_DATASET\", '').count(os.sep)\n",
        "        indent = '  ' * level\n",
        "        folder_name = os.path.basename(root) or \"A3CG_DATASET\"\n",
        "        print(f'{indent}- {folder_name}/')\n",
        "\n",
        "        sub_indent = '  ' * (level + 1)\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                size_kb = os.path.getsize(file_path) / 1024\n",
        "                print(f'{sub_indent}- {file} ({size_kb:.1f} KB)')\n",
        "\n",
        "    print(f\"\\nUse this path in your code:\")\n",
        "    print(f\"/content/A3CG_DATASET/folds/fold_1/\")\n",
        "\n",
        "else:\n",
        "    print(f\"FAILURE: Only {copied_count}/4 files were copied.\")\n",
        "    print(\"Please verify that all required JSON files are in your Google Drive folder.\")"
      ],
      "metadata": {
        "id": "E8tjEkZYwZGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace token : get it at https://huggingface.co/settings/tokens\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Interactive login\n",
        "login()\n",
        "\n",
        "# Verification\n",
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"Connected as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication error: {e}\")"
      ],
      "metadata": {
        "id": "ns7V6VMxwiLW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# INSTALLATION SCRIPT\n",
        "# ===================\n",
        "\n",
        "!pip install -q --upgrade transformers peft bitsandbytes accelerate torch datasets scikit-learn \"numpy<2.0\"\n",
        "\n",
        "print(\"Installation completed.\")\n",
        "\n",
        "print(\"Installation successful with compatible versions.\")\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"1. RESTART THE RUNTIME (MANDATORY)\")\n",
        "print(\"   Runtime > Restart session\")\n",
        "print(\"2. Wait 30 seconds after restart\")\n",
        "\n",
        "print(\"\\nWHY THIS RESTART IS CRITICAL:\")\n",
        "print(\"   - Prevents NumPy 1.x/2.x conflicts in memory\")\n",
        "print(\"   - Clears Python cache\")\n",
        "print(\"   - Ensures correct versions are loaded\")"
      ],
      "metadata": {
        "id": "UAaHlDwswmPx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: DEPENDENCIES INSTALLATION\n",
        "# Run this cell, then manually restart the runtime environment.\n",
        "\n",
        "print(\"STEP 0: Installing and upgrading required packages...\")\n",
        "\n",
        "# Install PyTorch from official CUDA source (cu121) to ensure compatibility and fix torchvision::nms error\n",
        "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install other required packages\n",
        "!pip install -q -U transformers peft bitsandbytes accelerate datasets\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "print(\"\\nINSTALLATION COMPLETE. PLEASE RESTART THE RUNTIME NOW.\")\n",
        "print(\"(Runtime -> Restart session)\")"
      ],
      "metadata": {
        "id": "1Xn-mz0qwoar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LORA + FEW-SHOT + CONTRASTIVE LEARNING (WITHOUT ORDINAL)\n",
        "\n",
        "# STEP 1: Imports\n",
        "print(\"\\nSTEP 1: Packages import...\")\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, Trainer, DataCollatorForLanguageModeling, TrainerCallback\n",
        "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
        "\n",
        "try:\n",
        "    import bitsandbytes\n",
        "    print(f\"  bitsandbytes: Import OK\")\n",
        "except Exception as e:\n",
        "    print(f\"  bitsandbytes error: {e}\")\n",
        "\n",
        "import json\n",
        "import time\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "from datasets import Dataset\n",
        "from typing import Dict, List, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
        "\n",
        "# STEP 2: Google Drive mounting\n",
        "print(\"\\nSTEP 2: Google Drive mounting...\")\n",
        "from google.colab import drive\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"  Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Drive error: {e}\")\n",
        "\n",
        "# STEP 3: Data verification\n",
        "print(\"\\nSTEP 3: Data verification...\")\n",
        "dataset_base = \"/content/A3CG_DATASET/folds/fold_1\"\n",
        "\n",
        "required_files = [\"seen_train.json\", \"seen_val.json\", \"seen_test.json\", \"unseen_test.json\"]\n",
        "files_ok = True\n",
        "\n",
        "for filename in required_files:\n",
        "    filepath = os.path.join(dataset_base, filename)\n",
        "    if os.path.exists(filepath):\n",
        "        size_kb = os.path.getsize(filepath) / 1024\n",
        "        print(f\"  {filename}: {size_kb:.1f} KB\")\n",
        "    else:\n",
        "        print(f\"  {filename}: Missing\")\n",
        "        files_ok = False\n",
        "\n",
        "if not files_ok:\n",
        "    print(\"WARNING: Data files missing!\")\n",
        "    print(\"Run the data import script first\")\n",
        "    exit()\n",
        "\n",
        "# STEP 4: Contrastive Learning Components\n",
        "print(\"\\nSTEP 4: Contrastive Learning Components...\")\n",
        "\n",
        "@dataclass\n",
        "class ContrastiveSample:\n",
        "    \"\"\"Structure for contrastive learning samples\"\"\"\n",
        "    anchor_text: str\n",
        "    anchor_aspects: Dict\n",
        "    positive_text: str\n",
        "    positive_aspects: Dict\n",
        "    negative_text: str\n",
        "    negative_aspects: Dict\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"Contrastive loss for aspect-action learning\"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float = 0.1, margin: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor_emb, positive_emb, negative_emb, reduction='mean'):\n",
        "        \"\"\"\n",
        "        Compute contrastive loss\n",
        "        Args:\n",
        "            anchor_emb: Embeddings of anchor samples [batch_size, hidden_dim]\n",
        "            positive_emb: Embeddings of positive samples [batch_size, hidden_dim]\n",
        "            negative_emb: Embeddings of negative samples [batch_size, hidden_dim]\n",
        "            reduction: 'mean' for batch average, 'none' for per-sample losses\n",
        "        Returns:\n",
        "            loss: Scalar (if reduction='mean') or [batch_size] tensor (if reduction='none')\n",
        "        \"\"\"\n",
        "        # Normalize embeddings\n",
        "        anchor_emb = F.normalize(anchor_emb, p=2, dim=1)\n",
        "        positive_emb = F.normalize(positive_emb, p=2, dim=1)\n",
        "        negative_emb = F.normalize(negative_emb, p=2, dim=1)\n",
        "\n",
        "        # Compute similarities\n",
        "        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=1)\n",
        "        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=1)\n",
        "\n",
        "        # Contrastive loss with temperature scaling\n",
        "        pos_exp = torch.exp(pos_sim / self.temperature)\n",
        "        neg_exp = torch.exp(neg_sim / self.temperature)\n",
        "\n",
        "        # InfoNCE-style loss per sample\n",
        "        loss_per_sample = -torch.log(pos_exp / (pos_exp + neg_exp + 1e-8))\n",
        "\n",
        "        if reduction == 'mean':\n",
        "            return loss_per_sample.mean()\n",
        "        elif reduction == 'none':\n",
        "            return loss_per_sample\n",
        "        else:\n",
        "            raise ValueError(f\"reduction must be 'mean' or 'none', got {reduction}\")\n",
        "\n",
        "class ContrastiveDataGenerator:\n",
        "    \"\"\"Generates contrastive samples for training\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.action_groups = {\n",
        "            'implemented': [],\n",
        "            'planning': [],\n",
        "            'indeterminate': []\n",
        "        }\n",
        "\n",
        "    def group_samples_by_action(self, data: List[Dict]):\n",
        "        \"\"\"Group samples by their predominant action type\"\"\"\n",
        "        for sample in data:\n",
        "            aspects = sample.get('aspects', {})\n",
        "            action_counts = {'implemented': 0, 'planning': 0, 'indeterminate':0}\n",
        "\n",
        "            for aspect, actions in aspects.items():\n",
        "                for action in actions:\n",
        "                    action_lower = action.lower().strip()\n",
        "\n",
        "                    if action_lower == 'implemented':\n",
        "                        action_counts['implemented'] += 1\n",
        "                    elif action_lower == 'planning':\n",
        "                        action_counts['planning'] += 1\n",
        "                    elif action_lower == 'indeterminate':\n",
        "                        action_counts['indeterminate'] += 1\n",
        "                    else:\n",
        "                        # For unrecognized actions, categorize as indeterminate\n",
        "                        action_counts['indeterminate'] += 1\n",
        "\n",
        "            # Determine predominant action\n",
        "            if sum(action_counts.values()) > 0:\n",
        "                main_action = max(action_counts, key=action_counts.get)\n",
        "                self.action_groups[main_action].append(sample)\n",
        "            else:\n",
        "                self.action_groups['indeterminate'].append(sample)\n",
        "\n",
        "        print(f\" Grouped samples:\")\n",
        "        for action, samples in self.action_groups.items():\n",
        "            print(f\"  {action}: {len(samples)} samples\")\n",
        "\n",
        "    def create_contrastive_pairs(self, data: List[Dict], n_pairs: int = None) -> List[ContrastiveSample]:\n",
        "        \"\"\"Create contrastive learning pairs\"\"\"\n",
        "        if n_pairs is None:\n",
        "            n_pairs = len(data)\n",
        "\n",
        "        self.group_samples_by_action(data)\n",
        "        contrastive_samples = []\n",
        "\n",
        "        print(f\"  Creating {n_pairs} contrastive pairs...\")\n",
        "\n",
        "        valid_actions = set(self.action_groups.keys())\n",
        "\n",
        "        for i in range(n_pairs):\n",
        "            if i%100 == 0:\n",
        "                print(f\" Progress: {i}/{n_pairs}\")\n",
        "\n",
        "            # Select anchor\n",
        "            anchor = random.choice(data)\n",
        "            anchor_aspects = anchor.get('aspects', {})\n",
        "\n",
        "            if not anchor_aspects:\n",
        "                continue\n",
        "\n",
        "            # Determine anchor's main action\n",
        "            anchor_actions = []\n",
        "            for actions in anchor_aspects.values():\n",
        "                anchor_actions.extend(actions)\n",
        "\n",
        "            if not anchor_actions:\n",
        "                continue\n",
        "\n",
        "            valid_anchor_actions = [action for action in anchor_actions if action in valid_actions]\n",
        "\n",
        "            if not valid_anchor_actions:\n",
        "                anchor_main_action = 'indeterminate'\n",
        "            else:\n",
        "                anchor_main_action = max(set(valid_anchor_actions), key=valid_anchor_actions.count)\n",
        "\n",
        "            if anchor_main_action not in valid_actions:\n",
        "                anchor_main_action = 'indeterminate'\n",
        "\n",
        "            # Find positive (same action type, different text)\n",
        "            positive_candidates = [s for s in self.action_groups[anchor_main_action]\n",
        "                                 if s['text'] != anchor['text']]\n",
        "\n",
        "            if not positive_candidates:\n",
        "                all_candidates = []\n",
        "                for action_type, samples in self.action_groups.items():\n",
        "                    all_candidates.extend([s for s in samples if s['text'] != anchor['text']])\n",
        "\n",
        "                if all_candidates:\n",
        "                    positive = random.choice(all_candidates)\n",
        "                else:\n",
        "                    positive = anchor\n",
        "            else:\n",
        "                positive = random.choice(positive_candidates)\n",
        "\n",
        "            # Find negative (different action type)\n",
        "            negative_actions = [a for a in valid_actions if a != anchor_main_action]\n",
        "            if negative_actions:\n",
        "                neg_action = random.choice(negative_actions)\n",
        "                negative_candidates = self.action_groups[neg_action]\n",
        "                if negative_candidates:\n",
        "                    negative = random.choice(negative_candidates)\n",
        "                else:\n",
        "                    other_samples = [s for s in data if s['text'] != anchor['text']]\n",
        "                    negative = random.choice(other_samples) if other_samples else anchor\n",
        "            else:\n",
        "                other_samples = [s for s in data if s['text'] != anchor['text']]\n",
        "                negative = random.choice(other_samples) if other_samples else anchor\n",
        "\n",
        "            contrastive_samples.append(ContrastiveSample(\n",
        "                anchor_text=anchor['text'],\n",
        "                anchor_aspects=anchor_aspects,\n",
        "                positive_text=positive['text'],\n",
        "                positive_aspects=positive.get('aspects', {}),\n",
        "                negative_text=negative['text'],\n",
        "                negative_aspects=negative.get('aspects', {})\n",
        "            ))\n",
        "\n",
        "        print(f\"  Generated {len(contrastive_samples)} contrastive samples\")\n",
        "        return contrastive_samples\n",
        "\n",
        "print(\"Contrastive components configured\")\n",
        "\n",
        "# STEP 5: Enhanced monitoring callback\n",
        "print(\"\\nSTEP 5: Enhanced monitoring configuration...\")\n",
        "\n",
        "class ContrastiveMemoryMonitorCallback(TrainerCallback):\n",
        "    def __init__(self):\n",
        "        self.start_time = time.time()\n",
        "        self.last_check = time.time()\n",
        "\n",
        "    def on_step_end(self, args, state, control, model=None, **kwargs):\n",
        "        current_time = time.time()\n",
        "        step = state.global_step\n",
        "\n",
        "        if step % 10 == 0:\n",
        "            elapsed = current_time - self.start_time\n",
        "            step_time = current_time - self.last_check\n",
        "\n",
        "            if torch.cuda.is_available():\n",
        "                allocated = torch.cuda.memory_allocated() / 1e9\n",
        "                reserved = torch.cuda.memory_reserved() / 1e9\n",
        "\n",
        "                print(f\"Step {step}: GPU {allocated:.1f}/{reserved:.1f} GB, Time: {elapsed/60:.1f}min\")\n",
        "\n",
        "                # Auto cleanup\n",
        "                gpu_total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "                if reserved / gpu_total > 0.9:\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(\"Automatic cleanup\")\n",
        "            else:\n",
        "                print(f\"Step {step}: Time: {elapsed/60:.1f}min\")\n",
        "\n",
        "            self.last_check = current_time\n",
        "\n",
        "    def on_epoch_end(self, args, state, control, model=None, **kwargs):\n",
        "        epoch = state.epoch\n",
        "        elapsed = time.time() - self.start_time\n",
        "        print(f\"Epoch {epoch} completed in {elapsed/60:.1f} minutes\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "print(\"Enhanced callback configured\")\n",
        "\n",
        "# STEP 6: Model configuration with contrastive head\n",
        "print(\"\\nSTEP 6: Model configuration with contrastive capabilities...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(f\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\"\n",
        ")\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Loading LLaMA-3 8B model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={'': 0},\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "# LoRA preparation\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "# LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    bias=\"none\",\n",
        "    lora_dropout=0.1,\n",
        "    task_type=TaskType.CAUSAL_LM,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)\n",
        "\n",
        "class ContrastiveLoRAModel(nn.Module):\n",
        "    \"\"\"Wrapper model that combines generation and contrastive learning (without ordinal)\"\"\"\n",
        "\n",
        "    def __init__(self, base_model, hidden_size=4096, contrastive_dim=256):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.contrastive_dim = contrastive_dim\n",
        "\n",
        "        # Contrastive projection head\n",
        "        self.contrastive_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size // 2, contrastive_dim)\n",
        "        )\n",
        "\n",
        "        self.contrastive_loss_fn = ContrastiveLoss(temperature=0.1)\n",
        "\n",
        "    def get_text_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Extract text embedding for contrastive learning\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.base_model.base_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            # Use last hidden state with mean pooling\n",
        "            last_hidden = outputs.hidden_states[-1]\n",
        "\n",
        "            # Mean pooling with attention mask\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size())\n",
        "            sum_hidden = torch.sum(last_hidden * mask_expanded, dim=1)\n",
        "            sum_mask = torch.sum(mask_expanded, dim=1)\n",
        "            mean_hidden = sum_hidden / sum_mask\n",
        "\n",
        "            return mean_hidden\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None,\n",
        "        contrastive_anchors=None, contrastive_positives=None, contrastive_negatives=None,\n",
        "        lambda_base=0.1):\n",
        "\n",
        "        # Compute per-sample losses (generation + contrastive)\n",
        "\n",
        "        # 1. Generation loss per sample (not averaged)\n",
        "        generation_outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        # Calculate per-sample loss (cross-entropy per token, then average per sample)\n",
        "        if labels is not None:\n",
        "            logits = generation_outputs.logits\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            # Cross-entropy per token\n",
        "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "            token_losses = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1)\n",
        "            )\n",
        "            token_losses = token_losses.view(shift_labels.size())\n",
        "\n",
        "            # Mask to ignore padding tokens (-100)\n",
        "            mask = (shift_labels != -100).float()\n",
        "\n",
        "            # Generation loss per sample (average over valid tokens)\n",
        "            generation_loss_per_sample = (token_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n",
        "        else:\n",
        "            generation_loss_per_sample = torch.zeros(input_ids.size(0), device=input_ids.device)\n",
        "\n",
        "        # 2. Initialize contrastive loss\n",
        "        contrastive_loss_per_sample = torch.zeros_like(generation_loss_per_sample)\n",
        "\n",
        "        # 3. Contrastive loss per sample if data available\n",
        "        if contrastive_anchors is not None and contrastive_positives is not None and contrastive_negatives is not None:\n",
        "            # Get embeddings\n",
        "            anchor_emb = self.get_text_embedding(\n",
        "                contrastive_anchors['input_ids'],\n",
        "                contrastive_anchors['attention_mask']\n",
        "            )\n",
        "            positive_emb = self.get_text_embedding(\n",
        "                contrastive_positives['input_ids'],\n",
        "                contrastive_positives['attention_mask']\n",
        "            )\n",
        "            negative_emb = self.get_text_embedding(\n",
        "                contrastive_negatives['input_ids'],\n",
        "                contrastive_negatives['attention_mask']\n",
        "            )\n",
        "\n",
        "            # Project to contrastive space\n",
        "            anchor_proj = self.contrastive_head(anchor_emb)\n",
        "            positive_proj = self.contrastive_head(positive_emb)\n",
        "            negative_proj = self.contrastive_head(negative_emb)\n",
        "\n",
        "            contrastive_loss_per_sample = self.contrastive_loss_fn(\n",
        "                anchor_proj, positive_proj, negative_proj, reduction='none'\n",
        "            )\n",
        "\n",
        "        # Loss combination: generation + contrastive only\n",
        "        scaled_contr_loss = lambda_base * contrastive_loss_per_sample\n",
        "        total_loss_per_sample = generation_loss_per_sample + scaled_contr_loss\n",
        "        total_loss = total_loss_per_sample.mean()\n",
        "\n",
        "        # Optional loss monitoring\n",
        "        if hasattr(self, '_loss_monitoring'):\n",
        "            with torch.no_grad():\n",
        "                self._loss_monitoring = {\n",
        "                    'generation_loss_mean': generation_loss_per_sample.mean().item(),\n",
        "                    'contrastive_loss_mean': contrastive_loss_per_sample.mean().item(),\n",
        "                    'scaled_contrastive_mean': scaled_contr_loss.mean().item(),\n",
        "                    'lambda_base': lambda_base,\n",
        "                }\n",
        "\n",
        "        generation_outputs.loss = total_loss\n",
        "        return generation_outputs\n",
        "\n",
        "# Wrap model with contrastive capabilities\n",
        "model = ContrastiveLoRAModel(model)\n",
        "model.print_trainable_parameters = lambda: print(f\"Trainable parameters: LoRA + Contrastive head\")\n",
        "\n",
        "print(\"Contrastive LoRA model configured\")\n",
        "\n",
        "# STEP 7: Enhanced Few-Shot data processor with contrastive support\n",
        "print(\"\\nSTEP 7: Enhanced Few-Shot + Contrastive processor...\")\n",
        "\n",
        "class A3CGContrastiveFewShotDataProcessor:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.system_prompt = \"\"\"You are an expert in ESG analysis. Extract aspect-action pairs from sustainability statements.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- Extract EXACT terms from the input text\n",
        "- Do not paraphrase or interpret creatively\n",
        "- Use literal wording from the sentence\n",
        "- Focus on specific terms rather than general concepts\n",
        "\n",
        "DEFINITIONS:\n",
        "- Aspect: A sustainability-related entity, goal, sub-area, or activity (use exact wording)\n",
        "- Action: \"implemented\", \"planning\", or \"indeterminate\"\n",
        "\n",
        "OUTPUT FORMAT: (\"aspect1\", \"action1\"), (\"aspect2\", \"action2\"), ...\n",
        "If none: (\"no aspect\", \"no action\")\"\"\"\n",
        "\n",
        "        # Enhanced few-shot examples\n",
        "        self.few_shot_examples = [\n",
        "            {\n",
        "                \"text\": \"We have implemented solar panels to reduce energy consumption in our facilities.\",\n",
        "                \"output\": '(\"solar panels\", \"implemented\"), (\"energy consumption\", \"implemented\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"The company plans to improve workplace diversity initiatives next year.\",\n",
        "                \"output\": '(\"workplace diversity initiatives\", \"planning\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We are committed to enhancing our environmental management systems.\",\n",
        "                \"output\": '(\"environmental management systems\", \"planning\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Our recycling program has achieved a 50% waste reduction.\",\n",
        "                \"output\": '(\"recycling program\", \"implemented\"), (\"waste reduction\", \"implemented\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"The board may consider sustainability investments where feasible.\",\n",
        "                \"output\": '(\"sustainability investments\", \"indeterminate\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"All staff complete an e-learning programme from an accredited training provider.\",\n",
        "                \"output\": '(\"e-learning programme\", \"implemented\"), (\"training provider\", \"implemented\"), (\"staff\", \"implemented\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We strive to minimize carbon footprints in our operations.\",\n",
        "                \"output\": '(\"carbon footprints\", \"planning\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Recycled items are processed to create new products.\",\n",
        "                \"output\": '(\"recycled items\", \"indeterminate\")'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        self.contrastive_generator = ContrastiveDataGenerator()\n",
        "\n",
        "    def get_few_shot_examples(self, n_examples: int = 3) -> str:\n",
        "        \"\"\"Randomly selects n examples for few-shot\"\"\"\n",
        "        selected = random.sample(self.few_shot_examples, min(n_examples, len(self.few_shot_examples)))\n",
        "\n",
        "        examples_text = \"\"\n",
        "        for i, example in enumerate(selected, 1):\n",
        "            examples_text += f\"\\nExample {i}:\\n\"\n",
        "            examples_text += f\"Text: {example['text']}\\n\"\n",
        "            examples_text += f\"Output: {example['output']}\\n\"\n",
        "\n",
        "        return examples_text\n",
        "\n",
        "    def create_prompt(self, text: str, aspects_dict: Dict = None) -> str:\n",
        "        \"\"\"Create prompt using Llama 3 chat format via tokenizer\"\"\"\n",
        "        few_shot_text = self.get_few_shot_examples(n_examples=3)\n",
        "\n",
        "        user_content = (\n",
        "            f\"{few_shot_text.strip()}\\n\\n\"\n",
        "            f\"Now extract from this text:\\n\"\n",
        "            f\"Text: {text}\\n\\n\"\n",
        "            f\"Extract the aspect-action pairs:\"\n",
        "        )\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_content}\n",
        "        ]\n",
        "\n",
        "        if aspects_dict:\n",
        "            output_pairs = []\n",
        "            for aspect, actions in aspects_dict.items():\n",
        "                for action in actions:\n",
        "                    output_pairs.append(f'(\"{aspect}\", \"{action}\")')\n",
        "            expected_output = ', '.join(output_pairs) if output_pairs else '(\"no aspect\", \"no action\")'\n",
        "            messages.append({\"role\": \"assistant\", \"content\": expected_output})\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True if aspects_dict is None else False\n",
        "        )\n",
        "\n",
        "    def load_data(self, file_path: str) -> List[Dict]:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            return json.load(f)\n",
        "\n",
        "    def prepare_contrastive_dataset(self, data: List[Dict]) -> Tuple[Dataset, List[ContrastiveSample]]:\n",
        "        \"\"\"Prepare dataset with both generation and contrastive samples\"\"\"\n",
        "\n",
        "        print(f\"  Preparing {len(data)} samples with few-shot + contrastive...\")\n",
        "\n",
        "        # Generation prompts\n",
        "        prompts = []\n",
        "        for i, item in enumerate(data):\n",
        "            if i % 100 == 0:\n",
        "                print(f\"    Generation progress: {i}/{len(data)} ({i/len(data)*100:.1f}%)\")\n",
        "\n",
        "            prompt = self.create_prompt(item['text'], item.get('aspects', {}))\n",
        "            prompts.append(prompt)\n",
        "\n",
        "        # Contrastive samples\n",
        "        print(\"  Generating contrastive samples...\")\n",
        "        contrastive_samples = self.contrastive_generator.create_contrastive_pairs(data, n_pairs=len(data)//2)\n",
        "\n",
        "        print(f\"  {len(prompts)} generation prompts + {len(contrastive_samples)} contrastive samples\")\n",
        "\n",
        "        return Dataset.from_dict({\"text\": prompts}), contrastive_samples\n",
        "\n",
        "print(\"Enhanced processor configured\")\n",
        "\n",
        "# STEP 8: Data preparation with contrastive support\n",
        "print(\"\\nSTEP 8: Loading and preparing data with Few-Shot + Contrastive...\")\n",
        "\n",
        "processor = A3CGContrastiveFewShotDataProcessor(tokenizer=tokenizer)\n",
        "\n",
        "print(\"Loading JSON files...\")\n",
        "train_data = processor.load_data(f\"{dataset_base}/seen_train.json\")\n",
        "val_data = processor.load_data(f\"{dataset_base}/seen_val.json\")\n",
        "\n",
        "print(f\"Train: {len(train_data)} samples\")\n",
        "print(f\"Validation: {len(val_data)} samples\")\n",
        "\n",
        "# Prepare datasets\n",
        "print(\"Generating few-shot + contrastive data...\")\n",
        "train_dataset, train_contrastive = processor.prepare_contrastive_dataset(train_data)\n",
        "val_dataset, val_contrastive = processor.prepare_contrastive_dataset(val_data)\n",
        "\n",
        "# Display example\n",
        "print(\"\\nEXAMPLE GENERATION PROMPT:\")\n",
        "print(\"=\" * 50)\n",
        "sample_prompt = train_dataset[0]['text']\n",
        "print(sample_prompt[:800] + \"...\" if len(sample_prompt) > 800 else sample_prompt)\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(f\"\\nEXAMPLE CONTRASTIVE TRIPLET:\")\n",
        "print(\"=\" * 50)\n",
        "sample_contrastive = train_contrastive[0]\n",
        "print(f\"Anchor: {sample_contrastive.anchor_text[:100]}...\")\n",
        "print(f\"Positive: {sample_contrastive.positive_text[:100]}...\")\n",
        "print(f\"Negative: {sample_contrastive.negative_text[:100]}...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Tokenization\n",
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        truncation=True,\n",
        "        padding=False,\n",
        "        max_length=2048,\n",
        "        # max_length=1024,  # Use if VRAM is insufficient\n",
        "        return_tensors=None\n",
        "    )\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing data...\")\n",
        "train_dataset = train_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names,\n",
        "    desc=\"Train tokenization\"\n",
        ")\n",
        "val_dataset = val_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names,\n",
        "    desc=\"Validation tokenization\"\n",
        ")\n",
        "\n",
        "print(\"Data preparation completed\")\n",
        "\n",
        "# STEP 9: Enhanced data collator\n",
        "print(\"\\nSTEP 9: Configuring the enhanced contrastive data collator...\")\n",
        "\n",
        "class ContrastiveDataCollator:\n",
        "    \"\"\"\n",
        "    Custom data collator that handles standard padding for language modeling\n",
        "    and probabilistically injects tokenized contrastive learning triplets into each batch.\n",
        "    \"\"\"\n",
        "    def __init__(self, tokenizer, contrastive_samples: list, contrastive_prob: float = 0.3, contrastive_max_length: int = 512):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.contrastive_samples = contrastive_samples\n",
        "        self.contrastive_prob = contrastive_prob\n",
        "        self.contrastive_max_length = contrastive_max_length\n",
        "\n",
        "    def __call__(self, features: list) -> dict:\n",
        "        # Separate labels before padding\n",
        "        labels = [feature[\"labels\"] for feature in features] if \"labels\" in features[0] else None\n",
        "\n",
        "        # Use tokenizer's built-in padding\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "        )\n",
        "\n",
        "        # For causal LM, labels are the same as input_ids if not present\n",
        "        if labels is None:\n",
        "            batch[\"labels\"] = batch[\"input_ids\"].clone()\n",
        "        else:\n",
        "            batch[\"labels\"] = self.tokenizer.pad(\n",
        "                {\"input_ids\": labels},\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            )[\"input_ids\"]\n",
        "\n",
        "        # Probabilistically inject contrastive samples\n",
        "        if random.random() < self.contrastive_prob and self.contrastive_samples:\n",
        "            batch_size = batch['input_ids'].shape[0]\n",
        "            selected_contrastive = random.sample(self.contrastive_samples,\n",
        "                                               min(batch_size, len(self.contrastive_samples)))\n",
        "\n",
        "            # Prepare texts for tokenization\n",
        "            anchor_texts = [cs.anchor_text for cs in selected_contrastive]\n",
        "            positive_texts = [cs.positive_text for cs in selected_contrastive]\n",
        "            negative_texts = [cs.negative_text for cs in selected_contrastive]\n",
        "\n",
        "            # Tokenize triplets\n",
        "            anchor_tokens = self.tokenizer(anchor_texts, truncation=True, padding=True,\n",
        "                                         max_length=self.contrastive_max_length, return_tensors=\"pt\")\n",
        "            positive_tokens = self.tokenizer(positive_texts, truncation=True, padding=True,\n",
        "                                           max_length=self.contrastive_max_length, return_tensors=\"pt\")\n",
        "            negative_tokens = self.tokenizer(negative_texts, truncation=True, padding=True,\n",
        "                                           max_length=self.contrastive_max_length, return_tensors=\"pt\")\n",
        "\n",
        "            # Add tokenized triplets to batch\n",
        "            batch['contrastive_anchors'] = anchor_tokens\n",
        "            batch['contrastive_positives'] = positive_tokens\n",
        "            batch['contrastive_negatives'] = negative_tokens\n",
        "\n",
        "        return batch\n",
        "\n",
        "data_collator = ContrastiveDataCollator(\n",
        "    tokenizer=tokenizer,\n",
        "    contrastive_samples=train_contrastive,\n",
        "    contrastive_prob=0.3,\n",
        "    contrastive_max_length=512\n",
        ")\n",
        "\n",
        "print(\"Contrastive data collator configured.\")\n",
        "\n",
        "# STEP 10: Training configuration for Llama-3-8B\n",
        "print(\"\\nSTEP 10: Configuring training for Llama-3-8B Contrastive...\")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"/content/drive/MyDrive/A3CG_Llama3_8B_Contrastive_Simple_LoRA\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=100,\n",
        "    logging_steps=10,\n",
        "    learning_rate=3e-5,\n",
        "    weight_decay=0.01,\n",
        "    fp16=False,\n",
        "    bf16=True,\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=0.1,\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    eval_steps=100,\n",
        "    eval_strategy=\"steps\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    remove_unused_columns=False,\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "memory_callback = ContrastiveMemoryMonitorCallback()\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "trainer.add_callback(memory_callback)\n",
        "\n",
        "print(\"Trainer configured for Llama-3-8B Contrastive.\")\n",
        "\n",
        "# STEP 11: Training execution\n",
        "print(\"\\nSTEP 11: STARTING LLAMA-3-8B CONTRASTIVE + FEW-SHOT TRAINING...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"Start time: {time.strftime('%H:%M:%S')}\")\n",
        "print(f\"Base Model: {model_name}\")\n",
        "print(f\"Config: Batch {training_args.per_device_train_batch_size}x{training_args.gradient_accumulation_steps} (Effective: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}), Epochs {training_args.num_train_epochs}\")\n",
        "print(f\"LoRA: Rank {lora_config.r}, Alpha {lora_config.lora_alpha}\")\n",
        "print(f\"Few-Shot: 3 examples per prompt\")\n",
        "print(f\"Contrastive: {data_collator.contrastive_prob*100}% of batches, temperature={model.contrastive_loss_fn.temperature}\")\n",
        "print(f\"Architecture: Generation + Contrastive (without ordinal)\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    trainer.train()\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    print(f\"\\nLLAMA-3-8B CONTRASTIVE + FEW-SHOT TRAINING COMPLETED!\")\n",
        "    print(f\"Total time: {training_time/3600:.1f}h ({training_time/60:.1f}min)\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nERROR DURING TRAINING: {e}\")\n",
        "    print(\"Automatic cleanup...\")\n",
        "    torch.cuda.empty_cache()\n",
        "    raise e\n",
        "\n",
        "# STEP 12: Model saving\n",
        "model_output_path = f\"./Llama3-8B-lora-a3cg-contrastive-simple-final\"\n",
        "print(f\"\\nSTEP 12: Saving fine-tuned model to {model_output_path}...\")\n",
        "\n",
        "try:\n",
        "    trainer.model.base_model.save_pretrained(model_output_path)\n",
        "    tokenizer.save_pretrained(model_output_path)\n",
        "\n",
        "    torch.save(trainer.model.contrastive_head.state_dict(),\n",
        "               f\"{model_output_path}/contrastive_head.pt\")\n",
        "\n",
        "    print(f\"Model saved successfully in: {model_output_path}\")\n",
        "except Exception as e:\n",
        "    print(f\"Save error: {e}\")\n",
        "\n",
        "# STEP 13: Backup to Google Drive\n",
        "print(\"\\nSTEP 13: AUTOMATIC BACKUP TO GOOGLE DRIVE...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "import shutil\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
        "\n",
        "try:\n",
        "    drive_folder = f\"/content/drive/MyDrive/A3CG_Llama3_8B_Models\"\n",
        "    os.makedirs(drive_folder, exist_ok=True)\n",
        "    print(f\"Drive backup folder: {drive_folder}\")\n",
        "\n",
        "    drive_model_path = f\"{drive_folder}/lora-a3cg-contrastive-simple_{timestamp}\"\n",
        "    shutil.copytree(model_output_path, drive_model_path)\n",
        "\n",
        "    files_copied = os.listdir(drive_model_path)\n",
        "    total_size_mb = sum(os.path.getsize(os.path.join(drive_model_path, f))\n",
        "                       for f in files_copied) / (1024*1024)\n",
        "\n",
        "    print(f\"Model successfully copied to: {drive_model_path}\")\n",
        "    print(f\"Total size: {total_size_mb:.1f} MB\")\n",
        "\n",
        "    metadata_file = f\"{drive_folder}/model_info_simple_{timestamp}.txt\"\n",
        "    with open(metadata_file, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"A3CG LORA + CONTRASTIVE + FEW-SHOT MODEL METADATA\\n\")\n",
        "        f.write(f\"=\" * 60 + \"\\n\")\n",
        "        f.write(f\"Creation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "        f.write(f\"Base Model: {model_name}\\n\")\n",
        "        f.write(f\"Techniques: Q-LoRA + Contrastive Learning + Few-Shot (without ordinal)\\n\")\n",
        "        f.write(f\"Task: A3CG aspect-action extraction\\n\")\n",
        "        f.write(f\"\\n--- TRAINING CONFIG ---\\n\")\n",
        "        f.write(f\"Epochs: {training_args.num_train_epochs}\\n\")\n",
        "        f.write(f\"Batch Size: {training_args.per_device_train_batch_size} (Effective: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps})\\n\")\n",
        "        f.write(f\"Learning Rate: {training_args.learning_rate}\\n\")\n",
        "        f.write(f\"\\n--- LoRA CONFIG ---\\n\")\n",
        "        f.write(f\"Rank (r): {lora_config.r}\\n\")\n",
        "        f.write(f\"Alpha: {lora_config.lora_alpha}\\n\")\n",
        "        f.write(f\"Target Modules: {lora_config.target_modules}\\n\")\n",
        "        f.write(f\"\\n--- CONTRASTIVE CONFIG ---\\n\")\n",
        "        f.write(f\"Temperature: {model.contrastive_loss_fn.temperature}\\n\")\n",
        "        f.write(f\"Batch Probability: {data_collator.contrastive_prob}\\n\")\n",
        "        f.write(f\"Architecture: Generation + Contrastive (2 components)\\n\")\n",
        "        f.write(f\"\\n--- OUTPUT ---\\n\")\n",
        "        f.write(f\"Model Size (MB): {total_size_mb:.1f}\\n\")\n",
        "        f.write(f\"Drive Path: {drive_model_path}\\n\")\n",
        "\n",
        "    print(f\"Metadata file created: {metadata_file}\")\n",
        "    print(\"=\" * 70)\n",
        "    print(\"LLAMA-3-8B CONTRASTIVE FINE-TUNED MODEL IS READY!\")\n",
        "    print(f\"Location: {drive_model_path}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Backup error: {e}\")\n",
        "\n",
        "# STEP 14: Final cleanup\n",
        "print(\"\\nSTEP 14: Final cleanup...\")\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "print(\"\\nCONTRASTIVE SCRIPT COMPLETED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"End time: {time.strftime('%H:%M:%S')}\")\n",
        "print(f\"Model saved to: {model_output_path}\")\n",
        "print(f\"Backed up to Drive at: {drive_model_path}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"=\" * 40)\n",
        "print(\"1. Run evaluation with the new Llama-3-8B contrastive model\")\n",
        "print(\"2. Compare performance: Full (3 losses) vs Simple (2 losses)\")\n",
        "print(\"3. Analyze if ordinal loss really helps or adds complexity\")\n",
        "print(\"4. Document findings on contrastive learning effectiveness\")\n",
        "print(\"=\" * 40)"
      ],
      "metadata": {
        "id": "TYgJxa9xws8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A3CG MODEL EVALUATION SCRIPT - SIMPLE CONTRASTIVE\n",
        "# Exact Match evaluation compatible with Generation + Contrastive (without ordinal)\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"A3CG MODEL EVALUATION SCRIPT - SIMPLE CONTRASTIVE\")\n",
        "print(\"Exact Match evaluation (paper implementation)\")\n",
        "print(\"Compatible with Generation + Contrastive (without ordinal)\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"Start time: {time.strftime('%H:%M:%S')}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# STEP 1: Import exact classes from simple training\n",
        "print(\"STEP 1: Import exact classes from simple training...\")\n",
        "\n",
        "@dataclass\n",
        "class ContrastiveSample:\n",
        "    \"\"\"Structure for contrastive learning samples\"\"\"\n",
        "    anchor_text: str\n",
        "    anchor_aspects: Dict\n",
        "    positive_text: str\n",
        "    positive_aspects: Dict\n",
        "    negative_text: str\n",
        "    negative_aspects: Dict\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    \"\"\"Contrastive loss for aspect-action learning - exact copy from training\"\"\"\n",
        "\n",
        "    def __init__(self, temperature: float = 0.1, margin: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, anchor_emb, positive_emb, negative_emb, reduction='mean'):\n",
        "        \"\"\"Exact implementation from training\"\"\"\n",
        "        # Normalize embeddings\n",
        "        anchor_emb = F.normalize(anchor_emb, p=2, dim=1)\n",
        "        positive_emb = F.normalize(positive_emb, p=2, dim=1)\n",
        "        negative_emb = F.normalize(negative_emb, p=2, dim=1)\n",
        "\n",
        "        # Compute similarities\n",
        "        pos_sim = F.cosine_similarity(anchor_emb, positive_emb, dim=1)\n",
        "        neg_sim = F.cosine_similarity(anchor_emb, negative_emb, dim=1)\n",
        "\n",
        "        # Contrastive loss with temperature scaling\n",
        "        pos_exp = torch.exp(pos_sim / self.temperature)\n",
        "        neg_exp = torch.exp(neg_sim / self.temperature)\n",
        "\n",
        "        # InfoNCE-style loss per sample\n",
        "        loss_per_sample = -torch.log(pos_exp / (pos_exp + neg_exp + 1e-8))\n",
        "\n",
        "        if reduction == 'mean':\n",
        "            return loss_per_sample.mean()\n",
        "        elif reduction == 'none':\n",
        "            return loss_per_sample\n",
        "        else:\n",
        "            raise ValueError(f\"reduction must be 'mean' or 'none', got {reduction}\")\n",
        "\n",
        "class ContrastiveLoRAModel(nn.Module):\n",
        "    \"\"\"Modified class for simple version (Generation + Contrastive only)\"\"\"\n",
        "\n",
        "    def __init__(self, base_model, hidden_size=4096, contrastive_dim=256):\n",
        "        super().__init__()\n",
        "        self.base_model = base_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.contrastive_dim = contrastive_dim\n",
        "\n",
        "        # Contrastive projection head\n",
        "        self.contrastive_head = nn.Sequential(\n",
        "            nn.Linear(hidden_size, hidden_size // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_size // 2, contrastive_dim)\n",
        "        )\n",
        "\n",
        "        self.contrastive_loss_fn = ContrastiveLoss(temperature=0.1)\n",
        "\n",
        "    def get_text_embedding(self, input_ids, attention_mask):\n",
        "        \"\"\"Extract text embedding - exact method from training\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.base_model.base_model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                output_hidden_states=True\n",
        "            )\n",
        "\n",
        "            last_hidden = outputs.hidden_states[-1]\n",
        "            mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden.size())\n",
        "            sum_hidden = torch.sum(last_hidden * mask_expanded, dim=1)\n",
        "            sum_mask = torch.sum(mask_expanded, dim=1)\n",
        "            mean_hidden = sum_hidden / sum_mask\n",
        "\n",
        "            return mean_hidden\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None,\n",
        "            contrastive_anchors=None, contrastive_positives=None, contrastive_negatives=None,\n",
        "            lambda_base=0.1):\n",
        "\n",
        "        # Compute per-sample losses (Generation + Contrastive)\n",
        "\n",
        "        # 1. Generation loss per sample\n",
        "        generation_outputs = self.base_model(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels\n",
        "        )\n",
        "\n",
        "        if labels is not None:\n",
        "            logits = generation_outputs.logits\n",
        "            shift_logits = logits[..., :-1, :].contiguous()\n",
        "            shift_labels = labels[..., 1:].contiguous()\n",
        "\n",
        "            loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "            token_losses = loss_fct(\n",
        "                shift_logits.view(-1, shift_logits.size(-1)),\n",
        "                shift_labels.view(-1)\n",
        "            )\n",
        "            token_losses = token_losses.view(shift_labels.size())\n",
        "\n",
        "            mask = (shift_labels != -100).float()\n",
        "            generation_loss_per_sample = (token_losses * mask).sum(dim=1) / (mask.sum(dim=1) + 1e-8)\n",
        "        else:\n",
        "            generation_loss_per_sample = torch.zeros(input_ids.size(0), device=input_ids.device)\n",
        "\n",
        "        # 2. Initialize contrastive loss\n",
        "        contrastive_loss_per_sample = torch.zeros_like(generation_loss_per_sample)\n",
        "\n",
        "        # 3. Contrastive loss per sample if data available\n",
        "        if contrastive_anchors is not None and contrastive_positives is not None and contrastive_negatives is not None:\n",
        "            anchor_emb = self.get_text_embedding(\n",
        "                contrastive_anchors['input_ids'],\n",
        "                contrastive_anchors['attention_mask']\n",
        "            )\n",
        "            positive_emb = self.get_text_embedding(\n",
        "                contrastive_positives['input_ids'],\n",
        "                contrastive_positives['attention_mask']\n",
        "            )\n",
        "            negative_emb = self.get_text_embedding(\n",
        "                contrastive_negatives['input_ids'],\n",
        "                contrastive_negatives['attention_mask']\n",
        "            )\n",
        "\n",
        "            anchor_proj = self.contrastive_head(anchor_emb)\n",
        "            positive_proj = self.contrastive_head(positive_emb)\n",
        "            negative_proj = self.contrastive_head(negative_emb)\n",
        "\n",
        "            contrastive_loss_per_sample = self.contrastive_loss_fn(\n",
        "                anchor_proj, positive_proj, negative_proj, reduction='none'\n",
        "            )\n",
        "\n",
        "        # Simplified combination: generation + contrastive only\n",
        "        scaled_contr_loss = lambda_base * contrastive_loss_per_sample\n",
        "        total_loss_per_sample = generation_loss_per_sample + scaled_contr_loss\n",
        "        total_loss = total_loss_per_sample.mean()\n",
        "\n",
        "        generation_outputs.loss = total_loss\n",
        "        return generation_outputs\n",
        "\n",
        "    def generate(self, *args, **kwargs):\n",
        "        \"\"\"Forward to base model's generate method\"\"\"\n",
        "        return self.base_model.generate(*args, **kwargs)\n",
        "\n",
        "print(\"Exact classes imported from simple training\")\n",
        "\n",
        "# STEP 2: Robust model loading\n",
        "print(\"\\nSTEP 2: Robust model loading...\")\n",
        "\n",
        "# Configuration\n",
        "MODEL_BASE = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Configure data paths\n",
        "DATA_DIR = \"/content/A3CG_DATASET/folds/fold_1\"\n",
        "DRIVE_DATA_DIR = \"/content/drive/MyDrive/A3CG_Dataset/folds/fold_1\"\n",
        "\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"Using data directory: {DATA_DIR}\")\n",
        "elif os.path.exists(DRIVE_DATA_DIR):\n",
        "    DATA_DIR = DRIVE_DATA_DIR\n",
        "    print(f\"Using Drive data directory: {DATA_DIR}\")\n",
        "else:\n",
        "    print(\"Data directory not found!\")\n",
        "    print(\"Please ensure data is available at:\")\n",
        "    print(\"- /content/A3CG_DATASET/folds/fold_1 OR\")\n",
        "    print(\"- /content/drive/MyDrive/A3CG_Dataset/folds/fold_1\")\n",
        "\n",
        "def load_trained_model_robust(model_path: str):\n",
        "    \"\"\"Robust loading with thorough verification\"\"\"\n",
        "\n",
        "    print(f\"Analyzing trained model: {model_path}\")\n",
        "\n",
        "    # 1. Verify directory exists and contains required files\n",
        "    if not os.path.exists(model_path):\n",
        "        raise FileNotFoundError(f\"Model not found: {model_path}\")\n",
        "\n",
        "    files_in_model = os.listdir(model_path)\n",
        "    print(f\"Files found: {files_in_model}\")\n",
        "\n",
        "    required_files = ['adapter_config.json']\n",
        "    has_weights = any(f.endswith('.safetensors') or f.endswith('.bin') for f in files_in_model)\n",
        "\n",
        "    missing_files = [f for f in required_files if f not in files_in_model]\n",
        "    if missing_files:\n",
        "        print(f\"Missing files: {missing_files}\")\n",
        "\n",
        "    if not has_weights:\n",
        "        print(\"No weight files found (.safetensors or .bin)\")\n",
        "\n",
        "    # 2. Load tokenizer\n",
        "    print(\"Loading tokenizer...\")\n",
        "    try:\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        print(\"Tokenizer loaded from model\")\n",
        "    except:\n",
        "        print(\"Fallback: tokenizer from base model\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(MODEL_BASE)\n",
        "\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # 3. Load base model with quantization\n",
        "    print(\"Loading base model...\")\n",
        "\n",
        "    quantization_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "    )\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_BASE,\n",
        "        quantization_config=quantization_config,\n",
        "        device_map=\"auto\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        trust_remote_code=True,\n",
        "        attn_implementation=\"flash_attention_2\"\n",
        "    )\n",
        "\n",
        "    # 4. Load LoRA adapters\n",
        "    print(\"Loading LoRA adapters...\")\n",
        "\n",
        "    lora_model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        model_path,\n",
        "        torch_dtype=torch.bfloat16\n",
        "    )\n",
        "\n",
        "    print(f\"LoRA adapters loaded\")\n",
        "\n",
        "    # 5. Create contrastive model with exact simple training architecture\n",
        "    print(\"Creating simple contrastive model...\")\n",
        "\n",
        "    hidden_size = base_model.config.hidden_size\n",
        "    print(f\"Detected hidden size: {hidden_size}\")\n",
        "\n",
        "    contrastive_model = ContrastiveLoRAModel(\n",
        "        base_model=lora_model,\n",
        "        hidden_size=hidden_size,\n",
        "        contrastive_dim=256\n",
        "    )\n",
        "\n",
        "    # 6. Load contrastive head with verification\n",
        "    contrastive_head_path = os.path.join(model_path, \"contrastive_head.pt\")\n",
        "\n",
        "    if os.path.exists(contrastive_head_path):\n",
        "        print(\"Loading simple contrastive head...\")\n",
        "\n",
        "        try:\n",
        "            state_dict = torch.load(contrastive_head_path, map_location='cpu')\n",
        "            print(f\"Contrastive weights loaded: {list(state_dict.keys())}\")\n",
        "\n",
        "            # Verify compatibility before loading\n",
        "            model_state_keys = set(contrastive_model.contrastive_head.state_dict().keys())\n",
        "            loaded_keys = set(state_dict.keys())\n",
        "\n",
        "            if model_state_keys == loaded_keys:\n",
        "                contrastive_model.contrastive_head.load_state_dict(state_dict)\n",
        "                device = next(contrastive_model.base_model.parameters()).device\n",
        "                contrastive_model.contrastive_head.to(device)\n",
        "                print(\"Simple contrastive head loaded successfully\")\n",
        "                return contrastive_model, tokenizer, \"contrastive_simple_loaded\"\n",
        "            else:\n",
        "                print(f\"Key incompatibility:\")\n",
        "                print(f\"   Model: {model_state_keys}\")\n",
        "                print(f\"   File: {loaded_keys}\")\n",
        "                print(\"Using model without contrastive head\")\n",
        "                return lora_model, tokenizer, \"lora_only\"\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading contrastive head: {e}\")\n",
        "            print(\"Using LoRA model only\")\n",
        "            return lora_model, tokenizer, \"lora_only\"\n",
        "    else:\n",
        "        print(\"No contrastive head found\")\n",
        "        return lora_model, tokenizer, \"lora_only\"\n",
        "\n",
        "# Find and load model\n",
        "print(\"Searching for trained simple model...\")\n",
        "\n",
        "possible_paths = [\n",
        "    \"./Llama3-8B-lora-a3cg-contrastive-simple-final\",\n",
        "    \"./Llama3-8B-lora-a3cg-contrastive-final\",\n",
        "    \"/content/drive/MyDrive/A3CG_Llama3_8B_Contrastive_Simple_LoRA\",\n",
        "    \"/content/drive/MyDrive/A3CG_Llama3_8B_Contrastive_LoRA\"\n",
        "]\n",
        "\n",
        "# Add dynamic folders\n",
        "for base_dir in [\"/content/drive/MyDrive/A3CG_Llama3_8B_Models\"]:\n",
        "    if os.path.exists(base_dir):\n",
        "        for folder in os.listdir(base_dir):\n",
        "            if \"contrastive\" in folder:\n",
        "                possible_paths.append(os.path.join(base_dir, folder))\n",
        "\n",
        "model_path = None\n",
        "for path in possible_paths:\n",
        "    if os.path.exists(path):\n",
        "        model_path = path\n",
        "        print(f\"Model found: {path}\")\n",
        "        break\n",
        "\n",
        "if not model_path:\n",
        "    print(\"No trained model found!\")\n",
        "    print(\"Paths checked:\")\n",
        "    for path in possible_paths:\n",
        "        print(f\"  - {path}\")\n",
        "    raise FileNotFoundError(\"No trained model found!\")\n",
        "\n",
        "# Load model\n",
        "model, tokenizer, loading_method = load_trained_model_robust(model_path)\n",
        "\n",
        "print(f\"Model loaded with method: {loading_method}\")\n",
        "\n",
        "# STEP 3: Architecture verification\n",
        "print(\"\\nSTEP 3: Architecture verification...\")\n",
        "\n",
        "def verify_model_architecture(model, loading_method):\n",
        "    \"\"\"Verify architecture matches simple training\"\"\"\n",
        "\n",
        "    print(\"Verifying simple model architecture...\")\n",
        "\n",
        "    if loading_method in [\"contrastive_simple_loaded\", \"contrastive_loaded\"]:\n",
        "        print(\"Contrastive simple model with loaded head\")\n",
        "\n",
        "        if hasattr(model, 'contrastive_head'):\n",
        "            print(f\"Contrastive head present\")\n",
        "\n",
        "            # Verify dimensions\n",
        "            head_layers = list(model.contrastive_head.children())\n",
        "            print(f\"Contrastive head architecture:\")\n",
        "            for i, layer in enumerate(head_layers):\n",
        "                if hasattr(layer, 'in_features') and hasattr(layer, 'out_features'):\n",
        "                    print(f\"   Layer {i}: {layer.in_features} -> {layer.out_features}\")\n",
        "                else:\n",
        "                    print(f\"   Layer {i}: {type(layer).__name__}\")\n",
        "\n",
        "            # Quick test\n",
        "            device = next(model.parameters()).device\n",
        "            test_tensor = torch.randn(1, model.hidden_size).to(device)\n",
        "            try:\n",
        "                output = model.contrastive_head(test_tensor)\n",
        "                print(f\"Contrastive head test successful: {test_tensor.shape} -> {output.shape}\")\n",
        "\n",
        "                # Verify no ordinal loss (simple version)\n",
        "                if not hasattr(model, 'ordinal_contrastive_loss_fn'):\n",
        "                    print(\"Simple architecture confirmed (no ordinal loss)\")\n",
        "                else:\n",
        "                    print(\"Full architecture detected (with ordinal loss)\")\n",
        "\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"Contrastive head test error: {e}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(\"Missing contrastive head!\")\n",
        "            return False\n",
        "\n",
        "    elif loading_method == \"lora_only\":\n",
        "        print(\"LoRA only model (without contrastive)\")\n",
        "        return True\n",
        "\n",
        "    else:\n",
        "        print(\"Unknown loading method\")\n",
        "        return False\n",
        "\n",
        "architecture_ok = verify_model_architecture(model, loading_method)\n",
        "\n",
        "if not architecture_ok:\n",
        "    print(\"ERROR: Architecture does not match training!\")\n",
        "    print(\"   The evaluated model will not be the correct model.\")\n",
        "else:\n",
        "    print(\"Architecture verified - model matches simple training\")\n",
        "\n",
        "# STEP 4: Evaluation prompt generator\n",
        "print(\"\\nSTEP 4: Evaluation prompt generator configuration...\")\n",
        "\n",
        "class A3CGEvaluationPromptGenerator:\n",
        "    def __init__(self, tokenizer, model_type: str = \"contrastive\"):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.model_type = model_type\n",
        "\n",
        "        self.system_prompt = \"\"\"You are an expert in ESG analysis. Extract aspect-action pairs from sustainability statements.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- Extract EXACT terms from the input text\n",
        "- Do not paraphrase or interpret creatively\n",
        "- Use literal wording from the sentence\n",
        "- Focus on specific terms rather than general concepts\n",
        "\n",
        "DEFINITIONS:\n",
        "- Aspect: A sustainability-related entity, goal, sub-area, or activity (use exact wording)\n",
        "- Action: \"implemented\", \"planning\", or \"indeterminate\"\n",
        "\n",
        "OUTPUT FORMAT: (\"aspect1\", \"action1\"), (\"aspect2\", \"action2\"), ...\n",
        "If none: (\"no aspect\", \"no action\")\"\"\"\n",
        "\n",
        "        self.few_shot_examples = [\n",
        "            {\n",
        "                \"text\": \"We have implemented solar panels to reduce energy consumption in our facilities.\",\n",
        "                \"output\": '(\"solar panels\", \"implemented\"), (\"energy consumption\", \"implemented\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"The company plans to improve workplace diversity initiatives next year.\",\n",
        "                \"output\": '(\"workplace diversity initiatives\", \"planning\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We are committed to enhancing our environmental management systems.\",\n",
        "                \"output\": '(\"environmental management systems\", \"planning\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Our recycling program has achieved a 50% waste reduction.\",\n",
        "                \"output\": '(\"recycling program\", \"implemented\"), (\"waste reduction\", \"implemented\")'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"The board may consider sustainability investments where feasible.\",\n",
        "                \"output\": '(\"sustainability investments\", \"indeterminate\")'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def get_few_shot_examples(self, n_examples: int = 3) -> str:\n",
        "        \"\"\"Get few-shot examples\"\"\"\n",
        "        selected = random.sample(self.few_shot_examples, min(n_examples, len(self.few_shot_examples)))\n",
        "\n",
        "        examples_text = \"\"\n",
        "        for i, example in enumerate(selected, 1):\n",
        "            examples_text += f\"\\nExample {i}:\\n\"\n",
        "            examples_text += f\"Text: {example['text']}\\n\"\n",
        "            examples_text += f\"Output: {example['output']}\\n\"\n",
        "\n",
        "        return examples_text\n",
        "\n",
        "    def create_prompt(self, sentence: str) -> str:\n",
        "        \"\"\"Create evaluation prompt using Llama 3 chat template\"\"\"\n",
        "\n",
        "        if self.model_type in [\"contrastive_simple_loaded\", \"contrastive_loaded\", \"contrastive\"]:\n",
        "            # Few-shot prompt format with chat template\n",
        "            few_shot_text = self.get_few_shot_examples(n_examples=3)\n",
        "\n",
        "            user_content = (\n",
        "                f\"{few_shot_text.strip()}\\n\\n\"\n",
        "                f\"Now extract from this text:\\nText: {sentence}\\n\\n\"\n",
        "                f\"Extract the aspect-action pairs:\"\n",
        "            )\n",
        "\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": self.system_prompt},\n",
        "                {\"role\": \"user\", \"content\": user_content}\n",
        "            ]\n",
        "\n",
        "            return self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            # Standard prompt format with chat template\n",
        "            messages = [\n",
        "                {\"role\": \"user\", \"content\": f\"Extract aspect-action pairs from the following ESG sentence.\\nFormat your response as JSON with \\\"aspect-action_pairs\\\" containing a list of objects with \\\"aspect\\\" and \\\"action\\\" fields.\\n\\nSentence: {sentence}\\n\\nResponse:\"}\n",
        "            ]\n",
        "\n",
        "            return self.tokenizer.apply_chat_template(\n",
        "                messages,\n",
        "                tokenize=False,\n",
        "                add_generation_prompt=True\n",
        "            )\n",
        "\n",
        "# Initialize prompt generator\n",
        "prompt_generator = A3CGEvaluationPromptGenerator(tokenizer, loading_method)\n",
        "\n",
        "print(f\"Prompt generator configured for {loading_method}\")\n",
        "\n",
        "# STEP 5: Generation and parsing functions\n",
        "print(\"\\nSTEP 5: Generation and parsing functions configuration...\")\n",
        "\n",
        "def generate_prediction_enhanced(model, tokenizer, sentence: str, model_type: str, max_length: int = 512) -> str:\n",
        "    \"\"\"Enhanced prediction generation optimized for Llama 3-8B\"\"\"\n",
        "\n",
        "    prompt = prompt_generator.create_prompt(sentence)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generation parameters optimized for Llama 3-8B\n",
        "    if model_type in [\"contrastive_simple_loaded\", \"contrastive_loaded\", \"contrastive\"]:\n",
        "        gen_params = {\n",
        "            \"max_new_tokens\": 150,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.1,\n",
        "            \"top_p\": 0.9,\n",
        "            \"repetition_penalty\": 1.1,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        }\n",
        "    else:\n",
        "        gen_params = {\n",
        "            \"max_new_tokens\": 200,\n",
        "            \"do_sample\": True,\n",
        "            \"temperature\": 0.1,\n",
        "            \"top_p\": 0.9,\n",
        "            \"pad_token_id\": tokenizer.eos_token_id,\n",
        "            \"eos_token_id\": tokenizer.eos_token_id,\n",
        "        }\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            outputs = model.generate(**inputs, **gen_params)\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Generation error: {e}\")\n",
        "            # Fallback generation\n",
        "            outputs = model.generate(\n",
        "                inputs['input_ids'],\n",
        "                max_new_tokens=150,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "def parse_prediction_enhanced(prediction: str, model_type: str) -> List[Tuple[str, str]]:\n",
        "    \"\"\"Enhanced parsing based on model type and expected format\"\"\"\n",
        "\n",
        "    pairs = []\n",
        "\n",
        "    if model_type in [\"contrastive_simple_loaded\", \"contrastive_loaded\", \"contrastive\"]:\n",
        "        # Parse tuple format: (\"aspect\", \"action\"), (\"aspect\", \"action\")\n",
        "        try:\n",
        "            # Find all tuple patterns\n",
        "            tuple_pattern = r'\\(\\s*[\"\\']([^\"\\']+)[\"\\']\\s*,\\s*[\"\\']([^\"\\']+)[\"\\']\\s*\\)'\n",
        "            matches = re.findall(tuple_pattern, prediction)\n",
        "\n",
        "            for aspect, action in matches:\n",
        "                aspect = aspect.strip()\n",
        "                action = action.strip()\n",
        "                if aspect and action and aspect != \"no aspect\" and action != \"no action\":\n",
        "                    pairs.append((aspect.lower(), action.lower()))\n",
        "\n",
        "            # If no tuples found, try simpler patterns\n",
        "            if not pairs:\n",
        "                simple_pattern = r'\\(\\s*([^,\\)]+)\\s*,\\s*([^,\\)]+)\\s*\\)'\n",
        "                simple_matches = re.findall(simple_pattern, prediction)\n",
        "\n",
        "                for aspect, action in simple_matches:\n",
        "                    aspect = aspect.strip().strip('\"\\'')\n",
        "                    action = action.strip().strip('\"\\'')\n",
        "                    if aspect and action and aspect != \"no aspect\" and action != \"no action\":\n",
        "                        pairs.append((aspect.lower(), action.lower()))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Tuple parsing error: {e}\")\n",
        "\n",
        "    else:\n",
        "        # Standard JSON parsing for base models\n",
        "        try:\n",
        "            if '{' in prediction and '}' in prediction:\n",
        "                json_start = prediction.find('{')\n",
        "                brace_count = 0\n",
        "                json_end = json_start\n",
        "\n",
        "                for i in range(json_start, len(prediction)):\n",
        "                    if prediction[i] == '{':\n",
        "                        brace_count += 1\n",
        "                    elif prediction[i] == '}':\n",
        "                        brace_count -= 1\n",
        "                        if brace_count == 0:\n",
        "                            json_end = i + 1\n",
        "                            break\n",
        "\n",
        "                if json_end > json_start:\n",
        "                    json_str = prediction[json_start:json_end]\n",
        "                    json_str = re.sub(r'[\\n\\r\\t]', ' ', json_str)\n",
        "                    json_str = re.sub(r'\\s+', ' ', json_str)\n",
        "\n",
        "                    data = json.loads(json_str)\n",
        "\n",
        "                    for key in [\"aspect-action_pairs\", \"aspect_action_pairs\", \"pairs\", \"results\"]:\n",
        "                        if key in data and isinstance(data[key], list):\n",
        "                            for pair in data[key]:\n",
        "                                if isinstance(pair, dict) and \"aspect\" in pair and \"action\" in pair:\n",
        "                                    aspect = str(pair[\"aspect\"]).strip()\n",
        "                                    action = str(pair[\"action\"]).strip()\n",
        "                                    if aspect and action:\n",
        "                                        pairs.append((aspect.lower(), action.lower()))\n",
        "                            break\n",
        "\n",
        "        except json.JSONDecodeError:\n",
        "            # Fallback regex for JSON-like patterns\n",
        "            json_pattern = r'\\{\\s*[\"\\']aspect[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']\\s*,\\s*[\"\\']action[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']\\s*\\}'\n",
        "            matches = re.findall(json_pattern, prediction, re.IGNORECASE)\n",
        "\n",
        "            for aspect, action in matches:\n",
        "                pairs.append((aspect.strip().lower(), action.strip().lower()))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "print(\"Generation and parsing functions configured\")\n",
        "\n",
        "# STEP 6: Evaluation functions - Exact Match only\n",
        "print(\"\\nSTEP 6: Evaluation functions configuration...\")\n",
        "\n",
        "def calculate_metrics_exact_match(predictions: List[List[Tuple[str, str]]],\n",
        "                                 ground_truth: List[List[Tuple[str, str]]]) -> Dict:\n",
        "    \"\"\"Calculate metrics with exact matching (as in A3CG paper)\"\"\"\n",
        "\n",
        "    print(\"Calculating Exact Match metrics (A3CG paper)...\")\n",
        "\n",
        "    exact_matches = 0\n",
        "    partial_matches = 0\n",
        "    total_pred_pairs = 0\n",
        "    total_true_pairs = 0\n",
        "\n",
        "    exact_true_positives = 0\n",
        "    exact_false_positives = 0\n",
        "    exact_false_negatives = 0\n",
        "\n",
        "    for pred_pairs, true_pairs in zip(predictions, ground_truth):\n",
        "        total_pred_pairs += len(pred_pairs)\n",
        "        total_true_pairs += len(true_pairs)\n",
        "\n",
        "        # Convert to sets for exact comparison\n",
        "        pred_set = set(pred_pairs)\n",
        "        true_set = set(true_pairs)\n",
        "\n",
        "        # Exact matches for this sample\n",
        "        matched_pairs = pred_set.intersection(true_set)\n",
        "\n",
        "        exact_true_positives += len(matched_pairs)\n",
        "        exact_false_positives += len(pred_set - true_set)\n",
        "        exact_false_negatives += len(true_set - pred_set)\n",
        "\n",
        "        # Sample-level exact match\n",
        "        if pred_set == true_set and len(pred_set) > 0:\n",
        "            exact_matches += 1\n",
        "\n",
        "        # Sample-level partial match\n",
        "        if len(matched_pairs) > 0:\n",
        "            partial_matches += 1\n",
        "\n",
        "    n_samples = len(predictions)\n",
        "\n",
        "    exact_match_accuracy = exact_matches / n_samples if n_samples > 0 else 0\n",
        "    partial_match_accuracy = partial_matches / n_samples if n_samples > 0 else 0\n",
        "\n",
        "    exact_precision = exact_true_positives / (exact_true_positives + exact_false_positives) if (exact_true_positives + exact_false_positives) > 0 else 0\n",
        "    exact_recall = exact_true_positives / (exact_true_positives + exact_false_negatives) if (exact_true_positives + exact_false_negatives) > 0 else 0\n",
        "    exact_f1_score = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'exact_match_accuracy': exact_match_accuracy,\n",
        "        'partial_match_accuracy': partial_match_accuracy,\n",
        "        'exact_precision': exact_precision,\n",
        "        'exact_recall': exact_recall,\n",
        "        'exact_f1_score': exact_f1_score,\n",
        "        'exact_true_positives': exact_true_positives,\n",
        "        'exact_false_positives': exact_false_positives,\n",
        "        'exact_false_negatives': exact_false_negatives,\n",
        "        'total_predictions': total_pred_pairs,\n",
        "        'total_ground_truth': total_true_pairs,\n",
        "    }\n",
        "\n",
        "def load_test_data_flexible(file_path: str) -> Tuple[List[str], List[List[Tuple[str, str]]]]:\n",
        "    \"\"\"Flexible data loading\"\"\"\n",
        "    print(f\"Loading: {os.path.basename(file_path)}\")\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    sentences = []\n",
        "    ground_truth = []\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, dict):\n",
        "                if 'text' in item and 'aspects' in item:\n",
        "                    sentence = item['text']\n",
        "                    pairs = []\n",
        "\n",
        "                    aspects_dict = item['aspects']\n",
        "                    if isinstance(aspects_dict, dict):\n",
        "                        for aspect, actions in aspects_dict.items():\n",
        "                            if isinstance(actions, list):\n",
        "                                for action in actions:\n",
        "                                    pairs.append((aspect.strip(), action.strip()))\n",
        "                            elif isinstance(actions, str):\n",
        "                                pairs.append((aspect.strip(), actions.strip()))\n",
        "\n",
        "                    sentences.append(sentence)\n",
        "                    ground_truth.append(pairs)\n",
        "\n",
        "    print(f\"Loaded {len(sentences)} samples with {sum(len(gt) for gt in ground_truth)} total pairs\")\n",
        "    return sentences, ground_truth\n",
        "\n",
        "print(\"Exact match evaluation functions configured\")\n",
        "\n",
        "# STEP 7: Load test data\n",
        "print(\"\\nSTEP 7: Loading test data...\")\n",
        "\n",
        "test_files = {\n",
        "    'seen_test': f\"{DATA_DIR}/seen_test.json\",\n",
        "    'unseen_test': f\"{DATA_DIR}/unseen_test.json\"\n",
        "}\n",
        "\n",
        "test_data = {}\n",
        "for name, file_path in test_files.items():\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            sentences, ground_truth = load_test_data_flexible(file_path)\n",
        "            if len(sentences) > 0:\n",
        "                test_data[name] = (sentences, ground_truth)\n",
        "                print(f\"{name}: {len(sentences)} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading {name}: {e}\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "if not test_data:\n",
        "    print(\"WARNING: No test files found - creating demo data\")\n",
        "    test_data['demo'] = (\n",
        "        [\"This company has implemented strong environmental policies to reduce carbon emissions.\"],\n",
        "        [[(\"environmental policies\", \"implemented\"), (\"carbon emissions\", \"implemented\")]]\n",
        "    )\n",
        "\n",
        "# STEP 8: Model evaluation\n",
        "print(\"\\nSTEP 8: Simple contrastive model evaluation...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Model evaluated: {model_path}\")\n",
        "print(f\"Loading method: {loading_method}\")\n",
        "print(f\"Architecture verified: {'Yes' if architecture_ok else 'No'}\")\n",
        "print(f\"Architecture: Generation + Contrastive only (without ordinal)\")\n",
        "print(f\"Metrics: Exact Match (paper implementation)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name, (sentences, ground_truth) in test_data.items():\n",
        "    print(f\"\\nEvaluation on {dataset_name}...\")\n",
        "    print(f\"Base Model: {MODEL_BASE}\")\n",
        "    print(f\"Model type: {loading_method}\")\n",
        "    print(f\"Number of samples: {len(sentences)}\")\n",
        "\n",
        "    predictions = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Evaluate on full dataset or subset for testing\n",
        "    n_test = min(len(sentences), 270)\n",
        "    test_sentences = sentences[:n_test]\n",
        "    test_ground_truth = ground_truth[:n_test]\n",
        "\n",
        "    print(f\"Testing on {n_test} samples...\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        if i % 25 == 0:\n",
        "            print(f\"   Progress: {i}/{n_test} ({i/n_test*100:.1f}%)\")\n",
        "\n",
        "        try:\n",
        "            # Generate prediction\n",
        "            prediction_text = generate_prediction_enhanced(model, tokenizer, sentence, loading_method)\n",
        "\n",
        "            # Parse prediction\n",
        "            pred_pairs = parse_prediction_enhanced(prediction_text, loading_method)\n",
        "            predictions.append(pred_pairs)\n",
        "\n",
        "            # Debug first few predictions\n",
        "            if i < 3:\n",
        "                print(f\"   Sample {i+1}: {len(pred_pairs)} pairs predicted\")\n",
        "                print(f\"   Raw output: {prediction_text[:100]}...\")\n",
        "                print(f\"   Parsed pairs: {pred_pairs}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Error sample {i}: {e}\")\n",
        "            predictions.append([])\n",
        "\n",
        "    evaluation_time = time.time() - start_time\n",
        "\n",
        "    # Calculate exact match metrics\n",
        "    print(f\"Calculating metrics...\")\n",
        "    exact_metrics = calculate_metrics_exact_match(predictions, test_ground_truth)\n",
        "\n",
        "    # Save results\n",
        "    results[dataset_name] = {\n",
        "        'exact_metrics': exact_metrics,\n",
        "        'evaluation_time': evaluation_time,\n",
        "        'samples_per_second': n_test / evaluation_time,\n",
        "        'predictions': predictions[:5],\n",
        "        'ground_truth': test_ground_truth[:5],\n",
        "        'model_type': loading_method,\n",
        "        'n_samples': n_test,\n",
        "        'base_model': MODEL_BASE\n",
        "    }\n",
        "\n",
        "    print(f\"Evaluation completed in {evaluation_time:.2f}s\")\n",
        "    print(f\"Speed: {n_test/evaluation_time:.2f} samples/sec\")\n",
        "\n",
        "# STEP 9: Results display\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS - SIMPLE CONTRASTIVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "for dataset_name, result in results.items():\n",
        "    exact_metrics = result['exact_metrics']\n",
        "\n",
        "    print(f\"\\nRESULTS - {dataset_name.upper()}\")\n",
        "    print(f\"Base model: {result['base_model']}\")\n",
        "    print(f\"Model type: {loading_method}\")\n",
        "    print(f\"Architecture: Generation + Simple Contrastive (2 components)\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"EXACT MATCH METRICS (A3CG paper implementation):\")\n",
        "    print(f\"   Exact Match Accuracy:     {exact_metrics['exact_match_accuracy']:.4f} ({exact_metrics['exact_match_accuracy']*100:.2f}%)\")\n",
        "    print(f\"   Exact Precision:          {exact_metrics['exact_precision']:.4f} ({exact_metrics['exact_precision']*100:.2f}%)\")\n",
        "    print(f\"   Exact Recall:             {exact_metrics['exact_recall']:.4f} ({exact_metrics['exact_recall']*100:.2f}%)\")\n",
        "    print(f\"   Exact F1-Score:           {exact_metrics['exact_f1_score']:.4f} ({exact_metrics['exact_f1_score']*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nPERFORMANCE:\")\n",
        "    print(f\"   Evaluation speed:         {result['samples_per_second']:.2f} samples/sec\")\n",
        "\n",
        "# STEP 10: Detailed examples\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED EXAMPLES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for dataset_name, result in results.items():\n",
        "    print(f\"\\nDETAILED EXAMPLES - {dataset_name.upper()}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    sentences, ground_truth = test_data[dataset_name]\n",
        "    predictions = result['predictions']\n",
        "\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Sentence: {sentences[i][:120]}...\")\n",
        "        print(f\"Ground truth: {ground_truth[i]}\")\n",
        "        print(f\"Prediction: {predictions[i]}\")\n",
        "\n",
        "        # Exact match analysis\n",
        "        pred_set = set(predictions[i])\n",
        "        true_set = set(ground_truth[i])\n",
        "        exact_matches = pred_set.intersection(true_set)\n",
        "\n",
        "        print(f\"Exact Match Analysis:\")\n",
        "        if exact_matches:\n",
        "            print(f\"   Exact matches: {exact_matches}\")\n",
        "        else:\n",
        "            print(f\"   No exact matches\")\n",
        "\n",
        "        if not predictions[i]:\n",
        "            print(f\"   WARNING: No prediction generated\")\n",
        "\n",
        "# STEP 11: Performance summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY LLAMA 3-8B SIMPLE CONTRASTIVE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate average metrics across datasets\n",
        "if results:\n",
        "    avg_exact_f1 = np.mean([r['exact_metrics']['exact_f1_score'] for r in results.values()])\n",
        "    avg_exact_precision = np.mean([r['exact_metrics']['exact_precision'] for r in results.values()])\n",
        "    avg_exact_recall = np.mean([r['exact_metrics']['exact_recall'] for r in results.values()])\n",
        "\n",
        "    print(f\"BASE MODEL: {MODEL_BASE}\")\n",
        "    print(f\"MODEL TYPE: {loading_method.upper()}\")\n",
        "    print(f\"MODEL EVALUATED: {os.path.basename(model_path)}\")\n",
        "    print(f\"ARCHITECTURE: Generation + Simple Contrastive (2 components)\")\n",
        "    print(f\"ARCHITECTURE VERIFIED: {'Yes' if architecture_ok else 'No'}\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"EXACT MATCH METRICS (A3CG paper):\")\n",
        "    print(f\"   F1-Score:        {avg_exact_f1:.4f} ({avg_exact_f1*100:.2f}%)\")\n",
        "    print(f\"   Precision:       {avg_exact_precision:.4f} ({avg_exact_precision*100:.2f}%)\")\n",
        "    print(f\"   Recall:          {avg_exact_recall:.4f} ({avg_exact_recall*100:.2f}%)\")\n",
        "\n",
        "    # Performance comparison with paper\n",
        "    print(f\"\\nCOMPARISON WITH A3CG PAPER:\")\n",
        "    print(f\"   Paper GRACE (best):          47.51% F1 (exact)\")\n",
        "    print(f\"   Our Exact Match:             {avg_exact_f1*100:.2f}% F1\")\n",
        "\n",
        "    if avg_exact_f1 >= 0.25:\n",
        "        print(f\"   Performance comparable to paper\")\n",
        "    else:\n",
        "        print(f\"   Performance below paper baseline\")\n",
        "\n",
        "# STEP 12: Save results\n",
        "print(f\"\\nSTEP 12: Save results...\")\n",
        "\n",
        "# Create results directory\n",
        "results_dir = f\"/content/drive/MyDrive/A3CG_Evaluation_Results\"\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "\n",
        "# Save complete results\n",
        "results_file = f\"{results_dir}/evaluation_results_simple_contrastive_{timestamp}.json\"\n",
        "\n",
        "# Prepare data for saving\n",
        "save_data = {\n",
        "    'timestamp': timestamp,\n",
        "    'model_info': {\n",
        "        'model_path': model_path,\n",
        "        'base_model': MODEL_BASE,\n",
        "        'loading_method': loading_method,\n",
        "        'architecture_verified': architecture_ok,\n",
        "        'architecture_type': 'simple_contrastive'\n",
        "    },\n",
        "    'evaluation_results': {}\n",
        "}\n",
        "\n",
        "for dataset_name, result in results.items():\n",
        "    save_data['evaluation_results'][dataset_name] = {\n",
        "        'exact_metrics': result['exact_metrics'],\n",
        "        'evaluation_time': result['evaluation_time'],\n",
        "        'samples_per_second': result['samples_per_second'],\n",
        "        'n_samples': result['n_samples'],\n",
        "        'examples': {\n",
        "            'predictions': result['predictions'],\n",
        "            'ground_truth': result['ground_truth']\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Save JSON\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(save_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# Create comprehensive markdown report\n",
        "report_file = f\"{results_dir}/evaluation_report_simple_contrastive_{timestamp}.md\"\n",
        "\n",
        "with open(report_file, 'w', encoding='utf-8') as f:\n",
        "    f.write(f\"# A3CG Simple Contrastive Evaluation Report - Llama 3-8B\\n\\n\")\n",
        "    f.write(f\"**Date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
        "\n",
        "    f.write(f\"## Model Configuration\\n\\n\")\n",
        "    f.write(f\"- **Base model**: {MODEL_BASE}\\n\")\n",
        "    f.write(f\"- **Model path**: {model_path}\\n\")\n",
        "    f.write(f\"- **Loading method**: {loading_method}\\n\")\n",
        "    f.write(f\"- **Architecture**: Generation + Simple Contrastive (2 components, without ordinal)\\n\")\n",
        "    f.write(f\"- **Architecture verified**: {'Yes' if architecture_ok else 'No'}\\n\\n\")\n",
        "\n",
        "    f.write(f\"## Evaluation Method\\n\\n\")\n",
        "    f.write(f\"### Exact Match (A3CG Paper)\\n\")\n",
        "    f.write(f\"- Strict matching of aspect-action pairs\\n\")\n",
        "    f.write(f\"- Conservative approach, comparable to original paper\\n\\n\")\n",
        "\n",
        "    f.write(f\"## Evaluation Results\\n\\n\")\n",
        "\n",
        "    for dataset_name, result in results.items():\n",
        "        exact_metrics = result['exact_metrics']\n",
        "\n",
        "        f.write(f\"### {dataset_name.upper()}\\n\\n\")\n",
        "\n",
        "        f.write(f\"#### Exact Match (Paper)\\n\")\n",
        "        f.write(f\"| Metric | Value | Percentage |\\n\")\n",
        "        f.write(f\"|--------|-------|-----------|\\n\")\n",
        "        f.write(f\"| Exact F1 | {exact_metrics['exact_f1_score']:.4f} | {exact_metrics['exact_f1_score']*100:.2f}% |\\n\")\n",
        "        f.write(f\"| Exact Precision | {exact_metrics['exact_precision']:.4f} | {exact_metrics['exact_precision']*100:.2f}% |\\n\")\n",
        "        f.write(f\"| Exact Recall | {exact_metrics['exact_recall']:.4f} | {exact_metrics['exact_recall']*100:.2f}% |\\n\\n\")\n",
        "\n",
        "print(f\"Complete results saved:\")\n",
        "print(f\"   JSON: {results_file}\")\n",
        "print(f\"   Report: {report_file}\")\n",
        "\n",
        "# CONCLUSION\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SIMPLE CONTRASTIVE EVALUATION COMPLETED!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"End time: {time.strftime('%H:%M:%S')}\")\n",
        "print(f\"Model evaluated: {os.path.basename(model_path)}\")\n",
        "print(f\"Method: {loading_method}\")\n",
        "print(f\"Architecture: Generation + Simple Contrastive (2 components)\")\n",
        "print(f\"Architecture verified: {'Yes' if architecture_ok else 'No'}\")\n",
        "print(f\"Datasets evaluated: {len(results)}\")\n",
        "\n",
        "if results:\n",
        "    avg_exact_f1 = np.mean([r['exact_metrics']['exact_f1_score'] for r in results.values()])\n",
        "\n",
        "    print(f\"F1-Score Exact (paper): {avg_exact_f1:.4f} ({avg_exact_f1*100:.2f}%)\")\n",
        "\n",
        "print(f\"Results saved in: {results_dir}\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "if not architecture_ok:\n",
        "    print(\"WARNING: Architecture could not be fully verified.\")\n",
        "    print(\"   Results may not reflect trained model performance.\")\n",
        "elif loading_method not in [\"contrastive_simple_loaded\", \"contrastive_loaded\"]:\n",
        "    print(\"WARNING: Contrastive head was not loaded.\")\n",
        "    print(\"   Evaluated model uses only LoRA adapters.\")\n",
        "else:\n",
        "    print(\"SUCCESS: Complete simple contrastive model evaluated correctly!\")\n",
        "\n",
        "print(f\"\\nFINAL SUMMARY:\")\n",
        "print(f\"   Exact Match F1 (paper comparable): {avg_exact_f1*100:.1f}%\")\n",
        "print(f\"   Simplified architecture: Generation + Contrastive (without ordinal)\")\n",
        "\n",
        "print(\"\\nTo analyze results in detail, check:\")\n",
        "print(f\"   {results_file}\")\n",
        "print(f\"   {report_file}\")"
      ],
      "metadata": {
        "id": "I39SJtgA03QJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}