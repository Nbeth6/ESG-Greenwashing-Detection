{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jgNaqH4C9sUJ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}\")\n",
        "else:\n",
        "  print(\"No GPU detected. Go to Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import of A3CG files\n",
        "\n",
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "\n",
        "print(\"IMPORTING A3CG FILES\")\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"Mounting Google Drive\")\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Drive mounted successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    exit()\n",
        "\n",
        "# Finding the fold_1 folder\n",
        "print(\"\\nSearching for the 'fold_1' folder...\")\n",
        "\n",
        "possible_path = \"/content/drive/MyDrive/A3CG_Dataset/folds/fold_1\"\n",
        "drive_fold_path = None\n",
        "\n",
        "if os.path.exists(possible_path):\n",
        "    drive_fold_path = possible_path\n",
        "    print(f\"Found: {possible_path}\")\n",
        "else:\n",
        "    print(\"Folder 'fold_1' not found.\")\n",
        "    exit()\n",
        "\n",
        "# List contents of the folder\n",
        "print(f\"\\nContents of: {drive_fold_path}\")\n",
        "files_in_folder = os.listdir(drive_fold_path)\n",
        "required_files = [\"seen_train.json\", \"seen_val.json\", \"seen_test.json\", \"unseen_test.json\"]\n",
        "\n",
        "for file in files_in_folder:\n",
        "    if file.endswith('.json'):\n",
        "        file_path = os.path.join(drive_fold_path, file)\n",
        "        size_kb = os.path.getsize(file_path) / 1024\n",
        "        status = \"[OK]\" if file in required_files else \"[INFO]\"\n",
        "        print(f\"  {status} {file} ({size_kb:.1f} KB)\")\n",
        "\n",
        "# Create local structure\n",
        "local_path = \"/content/A3CG_DATASET/folds/fold_1\"\n",
        "os.makedirs(local_path, exist_ok=True)\n",
        "print(f\"\\nDirectory created: {local_path}\")\n",
        "\n",
        "# Copy files\n",
        "print(\"\\nCopying files...\")\n",
        "copied_count = 0\n",
        "\n",
        "for filename in required_files:\n",
        "    source = os.path.join(drive_fold_path, filename)\n",
        "    dest = os.path.join(local_path, filename)\n",
        "\n",
        "    if os.path.exists(source):\n",
        "        try:\n",
        "            shutil.copy2(source, dest)\n",
        "            size_kb = os.path.getsize(dest) / 1024\n",
        "\n",
        "            # Check for valid JSON\n",
        "            with open(dest, 'r', encoding='utf-8') as f:\n",
        "                data = json.load(f)\n",
        "\n",
        "            print(f\"  [OK] {filename}: {len(data)} samples ({size_kb:.1f} KB)\")\n",
        "            copied_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  [ERROR] {filename}: {e}\")\n",
        "    else:\n",
        "        print(f\"  [MISSING] {filename} not found\")\n",
        "\n",
        "# Final result\n",
        "print(f\"\\nRESULT:\")\n",
        "print(f\"  Files copied: {copied_count}/4\")\n",
        "\n",
        "if copied_count >= 3:\n",
        "    print(f\"SUCCESS! Files are ready.\")\n",
        "    print(f\"Path: {local_path}\")\n",
        "\n",
        "    # Show final structure\n",
        "    print(f\"\\nFinal Directory Structure:\")\n",
        "    for root, dirs, files in os.walk(\"/content/A3CG_DATASET\"):\n",
        "        level = root.replace(\"/content/A3CG_DATASET\", '').count(os.sep)\n",
        "        indent = '  ' * level\n",
        "        folder_name = os.path.basename(root) or \"A3CG_DATASET\"\n",
        "        print(f'{indent}- {folder_name}/')\n",
        "\n",
        "        sub_indent = '  ' * (level + 1)\n",
        "        for file in files:\n",
        "            if file.endswith('.json'):\n",
        "                file_path = os.path.join(root, file)\n",
        "                size_kb = os.path.getsize(file_path) / 1024\n",
        "                print(f'{sub_indent}- {file} ({size_kb:.1f} KB)')\n",
        "\n",
        "    print(f\"\\nUse this path in your code:\")\n",
        "    print(f\"/content/A3CG_DATASET/folds/fold_1/\")\n",
        "\n",
        "else:\n",
        "    print(f\"FAILURE: Only {copied_count}/4 files were copied.\")\n",
        "    print(\"Please verify that all required JSON files are in your Google Drive folder.\")"
      ],
      "metadata": {
        "id": "mGst9BQY9v83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace token : get it at https://huggingface.co/settings/tokens\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Interactive login\n",
        "login()\n",
        "\n",
        "# Verification\n",
        "from huggingface_hub import whoami\n",
        "try:\n",
        "    user_info = whoami()\n",
        "    print(f\"Connected as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Authentication error: {e}\")"
      ],
      "metadata": {
        "id": "oeN63Hse9wAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================\n",
        "# INSTALLATION SCRIPT\n",
        "# ===================\n",
        "\n",
        "!pip install -q --upgrade transformers peft bitsandbytes accelerate torch datasets scikit-learn \"numpy<2.0\"\n",
        "\n",
        "print(\"Installation completed.\")\n",
        "\n",
        "print(\"Installation successful with compatible versions.\")\n",
        "print(\"\\nNEXT STEPS:\")\n",
        "print(\"1. RESTART THE RUNTIME (MANDATORY)\")\n",
        "print(\"   Runtime > Restart session\")\n",
        "print(\"2. Wait 30 seconds after restart\")\n",
        "\n",
        "print(\"\\nWHY THIS RESTART IS CRITICAL:\")\n",
        "print(\"   - Prevents NumPy 1.x/2.x conflicts in memory\")\n",
        "print(\"   - Clears Python cache\")\n",
        "print(\"   - Ensures correct versions are loaded\")"
      ],
      "metadata": {
        "id": "xp8gfsCC9wD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CELL 1: DEPENDENCIES INSTALLATION\n",
        "# Run this cell, then manually restart the runtime environment.\n",
        "\n",
        "print(\"STEP 0: Installing and upgrading required packages...\")\n",
        "\n",
        "# Install PyTorch from official CUDA source (cu121) to ensure compatibility and fix torchvision::nms error\n",
        "!pip install -q -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "# Install other required packages\n",
        "!pip install -q -U transformers peft bitsandbytes accelerate datasets\n",
        "!pip install -q flash-attn --no-build-isolation\n",
        "\n",
        "print(\"\\nINSTALLATION COMPLETE. PLEASE RESTART THE RUNTIME NOW.\")\n",
        "print(\"(Runtime -> Restart session)\")"
      ],
      "metadata": {
        "id": "R9dNWUlt9wIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NO TRAINING - BASELINE EVALUATION WITH FEW-SHOT\n",
        "\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "import re\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel, PeftConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# STEP 1: Model Loading\n",
        "print(\"\\nSTEP 1: Model loading...\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "\n",
        "print(f\"Loading tokenizer...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    padding_side=\"left\"\n",
        ")\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Loading LLaMA-3 8B model...\")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map={'': 0},\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\"\n",
        ")\n",
        "\n",
        "model = base_model\n",
        "\n",
        "print(\"Model configured\")\n",
        "\n",
        "DATA_DIR = \"/content/A3CG_DATASET/folds/fold_1\"\n",
        "\n",
        "# STEP 2: Few-shot data processor\n",
        "print(\"\\nSTEP 2: Few-shot data processor...\")\n",
        "\n",
        "class A3CGEvaluationPromptGenerator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.system_prompt = \"\"\"You are an expert in ESG analysis. Extract aspect-action pairs from sustainability statements.\n",
        "\n",
        "CRITICAL INSTRUCTIONS:\n",
        "- Extract EXACT terms from the input text\n",
        "- Do not paraphrase or interpret creatively\n",
        "- Use literal wording from the sentence\n",
        "- Focus on specific terms rather than general concepts\n",
        "\n",
        "DEFINITIONS:\n",
        "- Aspect: A sustainability-related entity, goal, sub-area, or activity (use exact wording)\n",
        "- Action: \"implemented\", \"planning\", or \"indeterminate\"\n",
        "\n",
        "OUTPUT FORMAT: (\"aspect1\", \"action1\"), (\"aspect2\", \"action2\"), ...\n",
        "If none: (\"no aspect\", \"no action\")\"\"\"\n",
        "\n",
        "        self.few_shot_examples = [\n",
        "            {\n",
        "                \"text\": \"The Group revitalised and rejuvenated five existing coffeeshops in FY2022 to improve customers' dining experience and hygiene standards.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"hygiene standards\", \"action\": \"implemented\"}, {\"aspect\": \"customers\\' dining experience\", \"action\": \"implemented\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"It is imperative that we manage our business prudently with high standard of corporate governance and integrity.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"corporate governance\", \"action\": \"indeterminate\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We are committed to improve on our occupational health and safety initiatives and conduct regular reviews of our programmes, processes, risk assessments and controls.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"occupational health and safety initiatives\", \"action\": \"planning\"}, {\"aspect\": \"risk assessments and controls\", \"action\": \"planning\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"This project will commence in 2022 and take place over the next 3 years.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": []}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"In 2020, we achieved a 44% reduction in carbon emissions intensity against 2007 levels, putting us on track to achieving our SBTi-validated target of 59% by 2030.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"carbon emissions intensity\", \"action\": \"implemented\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We have in place robust worksite inspection procedures and monthly audits to identify workplace hazards and ensure all activities comply with all Group and regulatory requirements.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"workplace hazards\", \"action\": \"implemented\"}, {\"aspect\": \"worksite inspection procedures\", \"action\": \"implemented\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Moving ahead, we are targeting to include automation in tracking order status and completion, routing and invoicing to improve customer experience.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"customer experience\", \"action\": \"planning\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"Continuous learning is necessary to help enhance workmen proficiency.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"workmen proficiency\", \"action\": \"indeterminate\"}, {\"aspect\": \"continuous learning\", \"action\": \"indeterminate\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We will continue to maintain or lower our energy and water consumption in FY2022.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"energy and water consumption\", \"action\": \"planning\"}]}'\n",
        "            },\n",
        "            {\n",
        "                \"text\": \"We are committed to foster a non-discriminatory workplace environment.\",\n",
        "                \"output\": '{\"aspect-action_pairs\": [{\"aspect\": \"non-discriminatory workplace environment\", \"action\": \"planning\"}]}'\n",
        "            }\n",
        "        ]\n",
        "\n",
        "    def get_few_shot_examples(self, n_examples: int = 3) -> str:\n",
        "        \"\"\"Get few-shot examples\"\"\"\n",
        "        selected = random.sample(self.few_shot_examples, min(n_examples, len(self.few_shot_examples)))\n",
        "\n",
        "        examples_text = \"\"\n",
        "        for i, example in enumerate(selected, 1):\n",
        "            examples_text += f\"\\nExample {i}:\\n\"\n",
        "            examples_text += f\"  Text: {example['text']}\\n\"\n",
        "            examples_text += f\"  Output: {example['output']}\\n\"\n",
        "\n",
        "        return examples_text\n",
        "\n",
        "    def create_prompt(self, sentence: str) -> str:\n",
        "        \"\"\"Create evaluation prompt using Llama 3 chat template with few-shot examples\"\"\"\n",
        "\n",
        "        # Get few-shot examples\n",
        "        examples_text = self.get_few_shot_examples(n_examples=3)\n",
        "\n",
        "        # Create comprehensive prompt with examples\n",
        "        user_content = f\"\"\"{self.system_prompt}\n",
        "\n",
        "{examples_text}\n",
        "\n",
        "Now extract aspect-action pairs from this new sentence:\n",
        "Text: {sentence}\n",
        "\n",
        "IMPORTANT:\n",
        "- Action must be EXACTLY one of: \"implemented\", \"planning\", or \"indeterminate\"\n",
        "- Extract sustainability-related aspects only (environmental, social, governance)\n",
        "- Use exact wording from the sentence for aspects\n",
        "- Format as JSON: {{\"aspect-action_pairs\": [{{\"aspect\": \"...\", \"action\": \"...\"}}, ...]}}\n",
        "\n",
        "Response:\"\"\"\n",
        "\n",
        "        messages = [\n",
        "            {\"role\": \"user\", \"content\": user_content}\n",
        "        ]\n",
        "\n",
        "        return self.tokenizer.apply_chat_template(\n",
        "            messages,\n",
        "            tokenize=False,\n",
        "            add_generation_prompt=True\n",
        "        )\n",
        "\n",
        "# Initialize prompt generator\n",
        "prompt_generator = A3CGEvaluationPromptGenerator(tokenizer)\n",
        "\n",
        "print(f\"Prompt generator configured\")\n",
        "\n",
        "# STEP 3: Generation function and parsing\n",
        "print(\"\\nSTEP 3: Configuring generation function and parsing...\")\n",
        "\n",
        "def generate_prediction_enhanced(model, tokenizer, sentence: str, max_length: int = 512) -> str:\n",
        "    \"\"\"Enhanced prediction generation optimized for Llama-3-8B\"\"\"\n",
        "\n",
        "    prompt = prompt_generator.create_prompt(sentence)\n",
        "\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Move to device\n",
        "    device = next(model.parameters()).device if hasattr(model, 'parameters') else 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generation parameters optimized for Llama 3-8B\n",
        "    gen_params = {\n",
        "        \"max_new_tokens\": 200,\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.1,\n",
        "        \"top_p\": 0.9,\n",
        "        \"pad_token_id\": tokenizer.eos_token_id,\n",
        "        \"eos_token_id\": tokenizer.eos_token_id,\n",
        "    }\n",
        "\n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        try:\n",
        "            outputs = model.generate(**inputs, **gen_params)\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Generation error: {e}\")\n",
        "            # Fallback generation\n",
        "            outputs = model.generate(\n",
        "                inputs['input_ids'],\n",
        "                max_new_tokens=150,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "    # Decode only the generated part\n",
        "    generated_ids = outputs[0][inputs['input_ids'].shape[1]:]\n",
        "    response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "    return response.strip()\n",
        "\n",
        "def parse_prediction_enhanced(prediction: str, model_type: str = \"base\") -> List[Tuple[str, str]]:\n",
        "    \"\"\"Enhanced parsing for JSON format\"\"\"\n",
        "\n",
        "    pairs = []\n",
        "    prediction = prediction.strip()\n",
        "\n",
        "    # Standard JSON parsing for base models\n",
        "    try:\n",
        "        if '{' in prediction and '}' in prediction:\n",
        "            # Find the JSON block\n",
        "            json_start = prediction.find('{')\n",
        "            brace_count = 0\n",
        "            json_end = json_start\n",
        "\n",
        "            for i in range(json_start, len(prediction)):\n",
        "                if prediction[i] == '{':\n",
        "                    brace_count += 1\n",
        "                elif prediction[i] == '}':\n",
        "                    brace_count -= 1\n",
        "                    if brace_count == 0:\n",
        "                        json_end = i + 1\n",
        "                        break\n",
        "\n",
        "            if json_end > json_start:\n",
        "                json_str = prediction[json_start:json_end]\n",
        "                json_str = re.sub(r'[\\n\\r\\t]', ' ', json_str)\n",
        "                json_str = re.sub(r'\\s+', ' ', json_str)\n",
        "\n",
        "                data = json.loads(json_str)\n",
        "\n",
        "                # Try different possible keys\n",
        "                for key in [\"aspect-action_pairs\", \"aspect_action_pairs\", \"pairs\", \"results\"]:\n",
        "                    if key in data and isinstance(data[key], list):\n",
        "                        for pair in data[key]:\n",
        "                            if isinstance(pair, dict) and \"aspect\" in pair and \"action\" in pair:\n",
        "                                aspect = str(pair[\"aspect\"]).strip().lower()\n",
        "                                action = str(pair[\"action\"]).strip().lower()\n",
        "\n",
        "                                # Validate action\n",
        "                                if action in [\"implemented\", \"planning\", \"indeterminate\"]:\n",
        "                                    if aspect and action:\n",
        "                                        pairs.append((aspect, action))\n",
        "                        break\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        print(f\"DEBUG: JSON parsing failed, trying regex fallback\")\n",
        "\n",
        "        # Fallback: regex for JSON-like patterns\n",
        "        json_pattern = r'\\{\\s*[\"\\']aspect[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']\\s*,\\s*[\"\\']action[\"\\']\\s*:\\s*[\"\\']([^\"\\']+)[\"\\']\\s*\\}'\n",
        "        matches = re.findall(json_pattern, prediction, re.IGNORECASE)\n",
        "\n",
        "        for aspect, action in matches:\n",
        "            aspect = aspect.strip().lower()\n",
        "            action = action.strip().lower()\n",
        "            if action in [\"implemented\", \"planning\", \"indeterminate\"]:\n",
        "                pairs.append((aspect, action))\n",
        "\n",
        "        # Additional fallback: look for tuple-like patterns\n",
        "        if not pairs:\n",
        "            tuple_pattern = r'\\(\\s*[\"\\']([^\"\\']+)[\"\\']\\s*,\\s*[\"\\']([^\"\\']+)[\"\\']\\s*\\)'\n",
        "            matches = re.findall(tuple_pattern, prediction)\n",
        "\n",
        "            for aspect, action in matches:\n",
        "                aspect = aspect.strip().lower()\n",
        "                action = action.strip().lower()\n",
        "                if action in [\"implemented\", \"planning\", \"indeterminate\"]:\n",
        "                    pairs.append((aspect, action))\n",
        "\n",
        "    return pairs\n",
        "\n",
        "print(\"Generation and parsing functions configured\")\n",
        "\n",
        "# STEP 4: Evaluation functions - Exact Match only\n",
        "print(\"\\nSTEP 4: Evaluation functions configuration...\")\n",
        "\n",
        "def calculate_metrics_exact_match(predictions: List[List[Tuple[str, str]]],\n",
        "                                 ground_truth: List[List[Tuple[str, str]]]) -> Dict:\n",
        "    \"\"\"Calculate metrics with exact matching (A3CG paper implementation)\"\"\"\n",
        "\n",
        "    print(\"Calculating Exact Match metrics (A3CG paper)...\")\n",
        "\n",
        "    exact_matches = 0\n",
        "    partial_matches = 0\n",
        "    total_pred_pairs = 0\n",
        "    total_true_pairs = 0\n",
        "\n",
        "    exact_true_positives = 0\n",
        "    exact_false_positives = 0\n",
        "    exact_false_negatives = 0\n",
        "\n",
        "    for pred_pairs, true_pairs in zip(predictions, ground_truth):\n",
        "        total_pred_pairs += len(pred_pairs)\n",
        "        total_true_pairs += len(true_pairs)\n",
        "\n",
        "        # Convert to sets for exact comparison\n",
        "        pred_set = set(pred_pairs)\n",
        "        true_set = set(true_pairs)\n",
        "\n",
        "        # Exact matches for this sample\n",
        "        matched_pairs = pred_set.intersection(true_set)\n",
        "\n",
        "        exact_true_positives += len(matched_pairs)\n",
        "        exact_false_positives += len(pred_set - true_set)\n",
        "        exact_false_negatives += len(true_set - pred_set)\n",
        "\n",
        "        # Sample-level exact match\n",
        "        if pred_set == true_set and len(pred_set) > 0:\n",
        "            exact_matches += 1\n",
        "\n",
        "        # Sample-level partial match\n",
        "        if len(matched_pairs) > 0:\n",
        "            partial_matches += 1\n",
        "\n",
        "    n_samples = len(predictions)\n",
        "\n",
        "    exact_match_accuracy = exact_matches / n_samples if n_samples > 0 else 0\n",
        "    partial_match_accuracy = partial_matches / n_samples if n_samples > 0 else 0\n",
        "\n",
        "    exact_precision = exact_true_positives / (exact_true_positives + exact_false_positives) if (exact_true_positives + exact_false_positives) > 0 else 0\n",
        "    exact_recall = exact_true_positives / (exact_true_positives + exact_false_negatives) if (exact_true_positives + exact_false_negatives) > 0 else 0\n",
        "    exact_f1_score = 2 * (exact_precision * exact_recall) / (exact_precision + exact_recall) if (exact_precision + exact_recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'exact_match_accuracy': exact_match_accuracy,\n",
        "        'partial_match_accuracy': partial_match_accuracy,\n",
        "        'exact_precision': exact_precision,\n",
        "        'exact_recall': exact_recall,\n",
        "        'exact_f1_score': exact_f1_score,\n",
        "        'exact_true_positives': exact_true_positives,\n",
        "        'exact_false_positives': exact_false_positives,\n",
        "        'exact_false_negatives': exact_false_negatives,\n",
        "        'total_predictions': total_pred_pairs,\n",
        "        'total_ground_truth': total_true_pairs,\n",
        "    }\n",
        "\n",
        "def load_test_data_flexible(file_path: str) -> Tuple[List[str], List[List[Tuple[str, str]]]]:\n",
        "    \"\"\"Flexible data loading\"\"\"\n",
        "    print(f\"Loading: {os.path.basename(file_path)}\")\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    sentences = []\n",
        "    ground_truth = []\n",
        "\n",
        "    if isinstance(data, list):\n",
        "        for item in data:\n",
        "            if isinstance(item, dict):\n",
        "                if 'text' in item and 'aspects' in item:\n",
        "                    sentence = item['text']\n",
        "                    pairs = []\n",
        "\n",
        "                    aspects_dict = item['aspects']\n",
        "                    if isinstance(aspects_dict, dict):\n",
        "                        for aspect, actions in aspects_dict.items():\n",
        "                            if isinstance(actions, list):\n",
        "                                for action in actions:\n",
        "                                    pairs.append((aspect.strip(), action.strip()))\n",
        "                            elif isinstance(actions, str):\n",
        "                                pairs.append((aspect.strip(), actions.strip()))\n",
        "\n",
        "                    sentences.append(sentence)\n",
        "                    ground_truth.append(pairs)\n",
        "\n",
        "    print(f\"Loaded {len(sentences)} samples with {sum(len(gt) for gt in ground_truth)} total pairs\")\n",
        "    return sentences, ground_truth\n",
        "\n",
        "print(\"Exact match evaluation functions configured\")\n",
        "\n",
        "# STEP 5: Load test data\n",
        "print(\"\\nSTEP 5: Loading test data...\")\n",
        "\n",
        "test_files = {\n",
        "    'seen_test': f\"{DATA_DIR}/seen_test.json\",\n",
        "    'unseen_test': f\"{DATA_DIR}/unseen_test.json\"\n",
        "}\n",
        "\n",
        "test_data = {}\n",
        "for name, file_path in test_files.items():\n",
        "    if os.path.exists(file_path):\n",
        "        try:\n",
        "            sentences, ground_truth = load_test_data_flexible(file_path)\n",
        "            if len(sentences) > 0:\n",
        "                test_data[name] = (sentences, ground_truth)\n",
        "                print(f\"{name}: {len(sentences)} samples\")\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR loading {name}: {e}\")\n",
        "    else:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "\n",
        "if not test_data:\n",
        "    print(\"WARNING: No test files found - creating demo data\")\n",
        "    test_data['demo'] = (\n",
        "        [\"This company has implemented strong environmental policies to reduce carbon emissions.\"],\n",
        "        [[(\"environmental policies\", \"implemented\"), (\"carbon emissions\", \"implemented\")]]\n",
        "    )\n",
        "\n",
        "# STEP 6: Model evaluation\n",
        "print(\"\\nSTEP 6: Model evaluation...\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Metrics: Exact Match (paper implementation)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name, (sentences, ground_truth) in test_data.items():\n",
        "    print(f\"\\nEvaluation on {dataset_name}...\")\n",
        "    print(f\"Number of samples: {len(sentences)}\")\n",
        "\n",
        "    predictions = []\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Evaluate on full dataset\n",
        "    n_test = len(sentences)\n",
        "    test_sentences = sentences[:n_test]\n",
        "    test_ground_truth = ground_truth[:n_test]\n",
        "\n",
        "    print(f\"Testing on {n_test} samples...\")\n",
        "\n",
        "    for i, sentence in enumerate(test_sentences):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"   Progress: {i}/{n_test} ({i/n_test*100:.1f}%)\")\n",
        "\n",
        "        try:\n",
        "            # Generate prediction\n",
        "            prediction_text = generate_prediction_enhanced(model, tokenizer, sentence)\n",
        "\n",
        "            # Parse prediction\n",
        "            pred_pairs = parse_prediction_enhanced(prediction_text)\n",
        "            predictions.append(pred_pairs)\n",
        "\n",
        "            # Debug first few predictions\n",
        "            if i < 3:\n",
        "                print(f\"   Sample {i+1}: {len(pred_pairs)} pairs predicted\")\n",
        "                print(f\"   Raw output: {prediction_text[:100]}...\")\n",
        "                print(f\"   Parsed pairs: {pred_pairs}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"WARNING: Error sample {i}: {e}\")\n",
        "            predictions.append([])\n",
        "\n",
        "    evaluation_time = time.time() - start_time\n",
        "\n",
        "    # Calculate exact match metrics\n",
        "    print(f\"Calculating metrics...\")\n",
        "    exact_metrics = calculate_metrics_exact_match(predictions, test_ground_truth)\n",
        "\n",
        "    # Save results\n",
        "    results[dataset_name] = {\n",
        "        'exact_metrics': exact_metrics,\n",
        "        'evaluation_time': evaluation_time,\n",
        "        'samples_per_second': n_test / evaluation_time,\n",
        "        'predictions': predictions[:5],\n",
        "        'ground_truth': test_ground_truth[:5],\n",
        "        'n_samples': n_test,\n",
        "        'base_model': model_name,\n",
        "    }\n",
        "\n",
        "    print(f\"Evaluation completed in {evaluation_time:.2f}s\")\n",
        "    print(f\"Speed: {n_test/evaluation_time:.2f} samples/sec\")\n",
        "\n",
        "# STEP 7: Results display\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EVALUATION RESULTS - EXACT MATCH\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "for dataset_name, result in results.items():\n",
        "    exact_metrics = result['exact_metrics']\n",
        "\n",
        "    print(f\"\\nRESULTS - {dataset_name.upper()}\")\n",
        "    print(f\"Base model: {result['base_model']}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "    print(f\"EXACT MATCH METRICS (A3CG paper implementation):\")\n",
        "    print(f\"   Exact Match Accuracy:     {exact_metrics['exact_match_accuracy']:.4f} ({exact_metrics['exact_match_accuracy']*100:.2f}%)\")\n",
        "    print(f\"   Exact Precision:          {exact_metrics['exact_precision']:.4f} ({exact_metrics['exact_precision']*100:.2f}%)\")\n",
        "    print(f\"   Exact Recall:             {exact_metrics['exact_recall']:.4f} ({exact_metrics['exact_recall']*100:.2f}%)\")\n",
        "    print(f\"   Exact F1-Score:           {exact_metrics['exact_f1_score']:.4f} ({exact_metrics['exact_f1_score']*100:.2f}%)\")\n",
        "\n",
        "    print(f\"\\nPERFORMANCE:\")\n",
        "    print(f\"   Evaluation speed:         {result['samples_per_second']:.2f} samples/sec\")\n",
        "\n",
        "# STEP 8: Detailed examples\n",
        "print(f\"\\n\" + \"=\"*80)\n",
        "print(\"DETAILED EXAMPLES ANALYSIS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for dataset_name, result in results.items():\n",
        "    print(f\"\\nDETAILED EXAMPLES - {dataset_name.upper()}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    sentences, ground_truth = test_data[dataset_name]\n",
        "    predictions = result['predictions']\n",
        "\n",
        "    for i in range(min(3, len(predictions))):\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Sentence: {sentences[i][:120]}...\")\n",
        "        print(f\"Ground truth: {ground_truth[i]}\")\n",
        "        print(f\"Prediction: {predictions[i]}\")\n",
        "\n",
        "        # Exact match analysis\n",
        "        pred_set = set(predictions[i])\n",
        "        true_set = set(ground_truth[i])\n",
        "        exact_matches = pred_set.intersection(true_set)\n",
        "\n",
        "        print(f\"Exact Match Analysis:\")\n",
        "        if exact_matches:\n",
        "            print(f\"   Exact matches: {exact_matches}\")\n",
        "        else:\n",
        "            print(f\"   No exact matches\")\n",
        "\n",
        "        if not predictions[i]:\n",
        "            print(f\"   WARNING: No prediction generated\")\n",
        "\n",
        "# STEP 9: Performance summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PERFORMANCE SUMMARY LLAMA 3-8B BASELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Calculate average metrics across datasets\n",
        "if results:\n",
        "    avg_exact_f1 = np.mean([r['exact_metrics']['exact_f1_score'] for r in results.values()])\n",
        "    avg_exact_precision = np.mean([r['exact_metrics']['exact_precision'] for r in results.values()])\n",
        "    avg_exact_recall = np.mean([r['exact_metrics']['exact_recall'] for r in results.values()])\n",
        "\n",
        "    print(f\"BASE MODEL: {model_name}\")\n",
        "    print(f\"EVALUATION TYPE: Few-Shot Baseline (No Training)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    print(f\"EXACT MATCH METRICS (A3CG paper):\")\n",
        "    print(f\"   F1-Score:        {avg_exact_f1:.4f} ({avg_exact_f1*100:.2f}%)\")\n",
        "    print(f\"   Precision:       {avg_exact_precision:.4f} ({avg_exact_precision*100:.2f}%)\")\n",
        "    print(f\"   Recall:          {avg_exact_recall:.4f} ({avg_exact_recall*100:.2f}%)\")\n",
        "\n",
        "    # Performance comparison with paper\n",
        "    print(f\"\\nCOMPARISON WITH A3CG PAPER:\")\n",
        "    print(f\"   Paper GRACE (best):          47.51% F1 (exact)\")\n",
        "    print(f\"   Our Baseline:                {avg_exact_f1*100:.2f}% F1\")\n",
        "\n",
        "    if avg_exact_f1 >= 0.25:\n",
        "        print(f\"   Baseline performance acceptable\")\n",
        "    else:\n",
        "        print(f\"   Baseline performance below expectations\")\n",
        "\n",
        "print(\"\\nBASELINE EVALUATION COMPLETED!\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"End time: {time.strftime('%H:%M:%S')}\")\n",
        "print(f\"Datasets evaluated: {len(results)}\")\n",
        "\n",
        "if results:\n",
        "    print(f\"Baseline F1-Score: {avg_exact_f1:.4f} ({avg_exact_f1*100:.2f}%)\")\n",
        "\n",
        "print(\"=\" * 70)\n",
        "\n",
        "print(f\"\\nFINAL SUMMARY:\")\n",
        "print(f\"   Exact Match F1 (baseline): {avg_exact_f1*100:.1f}%\")\n",
        "print(f\"   Evaluation type: Few-Shot Baseline (No Training)\")\n",
        "print(f\"   This serves as performance baseline for fine-tuned models\")"
      ],
      "metadata": {
        "id": "r_Q7JxBK9wLI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}